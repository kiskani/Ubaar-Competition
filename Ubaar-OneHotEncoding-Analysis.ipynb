{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mean_absolute_precision_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data      = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/train.csv')\n",
    "test_data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/test.csv')\n",
    "\n",
    "data      = data.dropna(axis = 0)\n",
    "\n",
    "test_data.loc[12577, 'distanceKM']      = 52\n",
    "test_data.loc[12577, 'taxiDurationMin'] = 50\n",
    "test_data.loc[13853, 'distanceKM']      = 500\n",
    "test_data.loc[13853, 'taxiDurationMin'] = 380\n",
    "\n",
    "all_data = pd.concat((data, test_data)) \n",
    "all_data['source']           = all_data['sourceLatitude']*all_data['sourceLongitude']\n",
    "all_data['destination']      = all_data['destinationLatitude']*all_data['destinationLongitude']\n",
    "\n",
    "min_price = min(all_data['price'])\n",
    "\n",
    "ntrain = data.shape[0]\n",
    "ntest  = test_data.shape[0]\n",
    "\n",
    "categorical_vars = ['date', 'SourceState', 'destinationState', 'vehicleType', 'vehicleOption']\n",
    "\n",
    "dummies_data = pd.get_dummies(all_data[categorical_vars])\n",
    "all_data[dummies_data.columns] = dummies_data[dummies_data.columns]\n",
    "all_data.drop(categorical_vars, axis=1, inplace=True)\n",
    "\n",
    "train    = all_data[:ntrain]\n",
    "test     = all_data[ntrain:]\n",
    "\n",
    "X = train.drop(['ID','price'],axis=1)\n",
    "y = train.price\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBF = lgb.LGBMRegressor(objective='regression',num_leaves=15,\n",
    "                          learning_rate=0.05, n_estimators=15000,\n",
    "                          max_bin = 1000, bagging_fraction = 0.6,\n",
    "                          bagging_freq = 5, feature_fraction = 0.25,\n",
    "                          feature_fraction_seed=9, bagging_seed=20,\n",
    "                          min_data_in_leaf = 11, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "GBSTF = GradientBoostingRegressor(n_estimators=3200, learning_rate=0.05,\n",
    "                                  max_depth=10, max_features='sqrt',\n",
    "                                  min_samples_leaf=15, min_samples_split=10, \n",
    "                                  loss='huber', random_state =5)\n",
    "\n",
    "XGBF = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                        learning_rate=0.05, max_depth=10, \n",
    "                        min_child_weight=1.7817, n_estimators=2200,\n",
    "                        reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                        subsample=0.5213, silent=1,\n",
    "                        random_state =5 , nthread = -1)\n",
    "\n",
    "BAGF  = BaggingRegressor(n_estimators=100, max_samples=1.0, max_features=1.0, random_state=5, verbose=1)\n",
    "DECF  = DecisionTreeRegressor(max_depth=15)\n",
    "RFSTF = RandomForestRegressor(n_estimators=20, criterion='mae', \n",
    "                              max_depth=4, max_features='sqrt',\n",
    "                              min_samples_leaf=5, min_samples_split =3, random_state = 42)\n",
    "\n",
    "KNNF  = KNeighborsClassifier(2)\n",
    "LASSF = Lasso(fit_intercept = True)\n",
    "ABSTF = AdaBoostRegressor(n_estimators=1000, learning_rate=0.05, loss='linear', random_state=5)\n",
    "\n",
    "SVRF = SVR()\n",
    "RDGF = Ridge()\n",
    "ENTF = make_pipeline(RobustScaler(), ElasticNet(alpha=0.8, l1_ratio=.9, random_state=3))\n",
    "\n",
    "models = { \"Gboost\": GBSTF, \"xgb\": XGBF, \"bagging\": BAGF, \"lgbm\": LGBF, \"dec_tree\": DECF, \"Random_forest\": RFSTF,\n",
    "          \"knn\": KNNF, \"elasticNet\": ENTF, \"ridge\": RDGF, \"lasso\": LASSF, \"AdaBoost\": ABSTF, \"SVR\": SVRF}\n",
    "\n",
    "for model_name in models:\n",
    "    model = models[model_name]\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_cols = X_train.columns.tolist()\n",
    "    X_val['y_' + model_name] = model.predict(X_val[train_cols])\n",
    "    score = mean_absolute_precision_error(X_val['y_' + model_name], y_val)\n",
    "    print(model_name, '%.2f' % float((time.time() - start_time)/60 ) +\" mins, score= \", '%.2f' % score)\n",
    "\n",
    "X_val.to_pickle('dataFrames/One_Hot_X_val.pkl')\n",
    "\n",
    "#Gboost        5.05 mins, score=  16.90\n",
    "#xgb           6.62 mins, score=  17.15\n",
    "#bagging       0.89 mins, score=  17.65\n",
    "#lgbm          1.13 mins, score=  18.60\n",
    "#dec_tree      0.01 mins, score=  21.83\n",
    "#Random_forest 6.01 mins, score=  32.82\n",
    "#knn           0.02 mins, score=  38.16\n",
    "#elasticNet    0.01 mins, score=  83.91\n",
    "#ridge         0.00 mins, score=  169.28\n",
    "#lasso         0.10 mins, score=  146.42\n",
    "#AdaBoost      1.29 mins, score=  30.73\n",
    "#SVR           3.07 mins, score=  73.94\n",
    "# KernelRidge and LinearRegression() take more than 1 hour to run! Don't Run them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.read_pickle('dataFrames/One_Hot_X_val.pkl')\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = ['y_Gboost', 'y_xgb', 'y_bagging', 'y_lgbm', 'y_dec_tree', 'y_Random_forest', 'y_knn',\n",
    "             'y_elasticNet', 'y_ridge', 'y_lasso', 'y_AdaBoost', 'y_SVR']\n",
    "\n",
    "best_cols = ['y_Gboost', 'y_xgb', 'y_bagging', 'y_lgbm']\n",
    "best_pred = X_val[list(best_cols)]\n",
    "best_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pred_cols:\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.scatter(y_val, X_val[col])\n",
    "    plt.title('Price vs '+ col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [XGBF, BAGF, LGBF]\n",
    "stregr     = StackingRegressor(regressors=regressors, meta_regressor=GBSTF)\n",
    "start_time = time.time()\n",
    "stregr.fit(X_train, y_train)\n",
    "y_pred = stregr.predict(X_val[train_cols])\n",
    "score  = mean_absolute_precision_error(y_pred, y_val)\n",
    "print(\"stackingRegressor model\", '%.2f' % float((time.time() - start_time)/60 ) +\" mins, score= \", '%.2f' % score)\n",
    "# stackingRegressor model 10.60 mins, score=  17.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=3):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                #instance.fit(X[train_index], y[train_index])\n",
    "                instance.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "                y_pred = instance.predict(X.iloc[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "stacked_averaged_models  = StackingAveragedModels(base_models = (XGBF, BAGF), meta_model = GBSTF)\n",
    "stacked_averaged_models.fit(X_train, y_train)\n",
    "y_pred = stacked_averaged_models.predict(X_val[train_cols])\n",
    "score  = mean_absolute_precision_error(y_pred, y_val)\n",
    "print(\"stacking Averaged models\", '%.2f' % float((time.time() - start_time)/60 ) +\" mins, score =\", '%.2f' % score)\n",
    "# stacking Averaged models 30.03 mins, score = 17.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current best model \n",
    "start_time = time.time()\n",
    "GBST = GradientBoostingRegressor(n_estimators=3200, learning_rate=0.05,\n",
    "                                  max_depth=10, max_features='sqrt',\n",
    "                                  min_samples_leaf=15, min_samples_split=10, \n",
    "                                  loss='huber', random_state =5)\n",
    "\n",
    "GBST.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "y_pred_test = GBST.predict(test.drop(['ID','price'],axis=1))\n",
    "\n",
    "print('%.2f' % float((time.time() - start_time)/60 ) +\" mins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/mohsenkiskani/Downloads/Ubaar/submissions/submission32.csv\"\n",
    "with open(filename,\"w+\") as outputfile:\n",
    "    outputfile.write(\"ID,price\\n\")\n",
    "    for i in range(y_pred_test.shape[0]):\n",
    "        outputfile.write(str(test_data.ID[i])+\",\"+str(int(np.ceil(y_pred_test[i])))+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
