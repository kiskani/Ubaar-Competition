{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_121\"; OpenJDK Runtime Environment (Zulu 8.20.0.5-macosx) (build 1.8.0_121-b15); OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-macosx) (build 25.121-b15, mixed mode)\n",
      "  Starting server from /anaconda3/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpsmiatx1r\n",
      "  JVM stdout: /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpsmiatx1r/h2o_mohsenkiskani_started_from_python.out\n",
      "  JVM stderr: /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpsmiatx1r/h2o_mohsenkiskani_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/Los_Angeles</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.11</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month and 3 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mohsenkiskani_ntf1lf</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.778 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.4 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       America/Los_Angeles\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.11\n",
       "H2O cluster version age:    1 month and 3 days\n",
       "H2O cluster name:           H2O_from_python_mohsenkiskani_ntf1lf\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.778 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.4 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import h2o\n",
    "\n",
    "h2o.init(max_mem_size = 2)           \n",
    "h2o.remove_all()               \n",
    "\n",
    "from h2o.estimators.deeplearning import H2OAutoEncoderEstimator, H2ODeepLearningEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.xgboost import H2OXGBoostEstimator\n",
    "from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n",
    "\n",
    "#help(h2o)\n",
    "#help(H2ODeepLearningEstimator)\n",
    "#help(h2o.import_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_func(labels, preds):\n",
    "    return np.mean(np.abs((preds - labels)/(labels))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_h2o_all = h2o.import_file(path = os.path.realpath(\"train.csv\"))\n",
    "test_h2o      = h2o.import_file(path = os.path.realpath(\"test.csv\"))\n",
    "\n",
    "train_h2o, val_h2o = train_h2o_all.split_frame(ratios = [0.8], seed = 7)\n",
    "\n",
    "X_cols = train_h2o_all.col_names[1:-1]\n",
    "y_cols = train_h2o_all.col_names[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_long_model_1 = H2ODeepLearningEstimator(hidden=[40,40,40,40,40], epochs = 10000)\n",
    "deep_long_model_1.train(X_cols, y_cols, train_h2o)\n",
    "pred_long_model_1 = deep_long_model_1.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_long_model_1.as_matrix())\n",
    "#21.88915821827038     [20,20,20,20,20]\n",
    "#21.812563482703755    [40,40,40,40,40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model = H2ODeepLearningEstimator(epochs=10)\n",
    "dl_model.train(X_cols, y_cols, train_h2o)\n",
    "pred = dl_model.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred.as_matrix())\n",
    "\n",
    "#22.15701987460977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = H2OGradientBoostingEstimator()\n",
    "gbm_model.train(X_cols, y_cols, train_h2o)\n",
    "pred = gbm_model.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred.as_matrix())\n",
    "\n",
    "#22.505163722860498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drf_model = H2ORandomForestEstimator(max_depth = 40,\n",
    "    nfolds = 3, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n",
    "drf_model.train(X_cols, y_cols, train_h2o)\n",
    "pred = drf_model.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred.as_matrix())\n",
    "\n",
    "#18.126878064811716\n",
    "#18.115947532453333 max_depth = 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model = H2OGeneralizedLinearEstimator()\n",
    "glm_model.fit(train_h2o[X_cols], train_h2o[y_cols])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [dl_model, gbm_model, drf_model, glm_model]\n",
    "m_names = [\"Deep Learning\", \"Gradient Boosted Method\", \"Distributed Random Forest\", \"Generalized Linear Model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_1 = H2ODeepLearningEstimator(epochs=1)\n",
    "dl_1.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "dl_250 = H2ODeepLearningEstimator(checkpoint=dl_1, epochs=250)\n",
    "dl_250.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "dl_500 = H2ODeepLearningEstimator(checkpoint=dl_250, epochs=500)\n",
    "dl_500.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "dl_750 = H2ODeepLearningEstimator(checkpoint=dl_500, epochs=750)\n",
    "dl_750.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "models_dl = [dl_1, dl_250, dl_500, dl_750]\n",
    "m_names_dl = [\"DL \" + str(int(model.get_params()['epochs']['actual_value'])) + \" Epochs\" for model in models_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1   = dl_1.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "pred_250 = dl_250.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "pred_500 = dl_500.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "pred_750 = dl_750.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_1.as_matrix()))\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_250.as_matrix()))\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_500.as_matrix()))\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_750.as_matrix()))\n",
    "\n",
    "#28.603573415730754\n",
    "#22.39875197740143\n",
    "#22.7802683842562\n",
    "#22.04537421795538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_l1 = H2ODeepLearningEstimator(hidden=[1000], epochs=500)\n",
    "dl_l1.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "dl_l2 = H2ODeepLearningEstimator(hidden=[200,200], epochs=500)\n",
    "dl_l2.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "dl_l3 = H2ODeepLearningEstimator(hidden=[42,42,42], epochs=500)\n",
    "dl_l3.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "dl_l4 = H2ODeepLearningEstimator(hidden=[11,13,17,19], epochs = 1000)\n",
    "dl_l4.train(X_cols, y_cols, train_h2o)\n",
    "\n",
    "models_network = [dl_l1, dl_l2, dl_l3, dl_l4]\n",
    "m_names_network = [\"1000\", \"200 x 200\", \"42 x 42 x 42\", \"11 x 13 x 17 x 19\"]\n",
    "\n",
    "\n",
    "# ###Activation Functions\n",
    "# Next, we compare between different activation functions, including one with 50% dropout regularization in the hidden layers:\n",
    "\n",
    "models_act = []\n",
    "m_names_act = []\n",
    "for i,method in enumerate([\"Tanh\",\"Maxout\",\"Rectifier\",\"RectifierWithDropout\"]):\n",
    "    models_act.append(H2ODeepLearningEstimator(activation=method, hidden=[100,100], epochs=1000))\n",
    "    models_act[i].train(X_cols, y_cols, train_h2o)\n",
    "    m_names_act.append(\"DL \"+ method + \" Activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_l1   = dl_l1.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "pred_l2   = dl_l2.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "pred_l3   = dl_l3.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "pred_l4   = dl_l4.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_l1.as_matrix()))\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_l2.as_matrix()))\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_l3.as_matrix()))\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_l4.as_matrix()))\n",
    "\n",
    "for i in range(4):\n",
    "    pred = models_act[i].predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "    print(m_names_act[i], \" score = \" , mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred.as_matrix()))\n",
    "    \n",
    "    \n",
    "#24.898107487754242\n",
    "#23.07006928025099\n",
    "#22.494132169347903\n",
    "#22.454395504979217\n",
    "#DL Tanh Activation  score =  24.994449602375052\n",
    "#DL Maxout Activation  score =  25.168343688143718\n",
    "#DL Rectifier Activation  score =  23.07230430482452\n",
    "#DL RectifierWithDropout Activation  score =  42.88493826250886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_l5 = H2ODeepLearningEstimator(hidden=[400,400], epochs=1000)\n",
    "dl_l5.train(X_cols, y_cols, train_h2o)\n",
    "pred_l5   = dl_l5.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_l5.as_matrix()))\n",
    "np.sum(pred_l5.as_matrix()<0)\n",
    "# 22.50300980342744\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_l6 = H2ODeepLearningEstimator(hidden=[40,40,40], epochs=1000)\n",
    "dl_l6.train(X_cols, y_cols, train_h2o)\n",
    "pred_l6 = dl_l6.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_l6.as_matrix()))\n",
    "np.sum(pred_l6.as_matrix()<0)\n",
    "#22.41066233203548\n",
    "#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_l7 = H2ODeepLearningEstimator(hidden=[10,10,10,10], epochs=1000)\n",
    "dl_l7.train(X_cols, y_cols, train_h2o)\n",
    "pred_l7 = dl_l7.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_l7.as_matrix()))\n",
    "np.sum(pred_l7.as_matrix()<0)\n",
    "#23.002932001739524\n",
    "#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d1 = H2ODeepLearningEstimator(model_id=\"model_d1\", epochs=1, variable_importances=True)\n",
    "model_d1.train(X_cols, y_cols, training_frame = train_h2o , validation_frame = val_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2 = H2OXGBoostEstimator(nfolds=3, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n",
    "model_d2.train(X_cols, y_cols, train_h2o)\n",
    "pred_d2 = model_d2.predict(val_h2o).as_data_frame(use_pandas=True)\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_d2.as_matrix()))\n",
    "# 20.443528867009622    None \n",
    "# 20.443176634720867   'sort_by_response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = H2OStackedEnsembleEstimator(model_id=\"my_ensemble\", training_frame=train_h2o, validation_frame=val_h2o,\n",
    "                                    base_models= [model_d2.model_id, drf_model.model_id])\n",
    "stack.train(X_cols, y_cols, train_h2o)\n",
    "pred_d3 = stack.predict(val_h2o).as_data_frame(use_pandas=True)\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_d3.as_matrix()))\n",
    "stack.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.shutdown(prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using updated data\n",
    "train_h2o_all = h2o.import_file(path = os.path.realpath(\"dataFrames/train_updated_June27.csv\"))\n",
    "test_h2o      = h2o.import_file(path = os.path.realpath(\"dataFrames/test_updated_June27.csv\"))\n",
    "\n",
    "train_h2o, val_h2o = train_h2o_all.split_frame(ratios = [0.8], seed = 7)\n",
    "\n",
    "X_cols = train_h2o_all.col_names[1:-1]\n",
    "y_cols = train_h2o_all.col_names[5]\n",
    "\n",
    "X_cols = ['destinationLatitude', 'destinationLongitude', 'distanceKM', 'sourceLatitude', 'sourceLongitude',\n",
    "          'taxiDurationMin', 'weight', 'source', 'destination', 'Ridge', 'KNeighborsRegressor', 'Lasso', 'Pipeline',\n",
    "          'XGBRegressor', 'GradientBoostingRegressor', 'BaggingRegressor', 'DecisionTreeRegressor', 'AdaBoostRegressor',\n",
    "          'ExtraTreesRegressor', 'RandomForestRegressor', 'LGBMRegressor']\n",
    "\n",
    "X_cols = train_h2o_all.col_names \n",
    "X_cols.remove('C1')\n",
    "X_cols.remove('ID') \n",
    "X_cols.remove('price')\n",
    "len(X_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_long_model_1 = H2ODeepLearningEstimator(hidden=[40,40,40], epochs = 1000)\n",
    "deep_long_model_1.train(X_cols, y_cols, train_h2o)\n",
    "pred_long_model_1 = deep_long_model_1.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_long_model_1.as_matrix())\n",
    "# 19.1373295048555     [40,40,40]   all features \n",
    "#    [40,40,40]   only 9 continuous and 12 predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drf_model = H2ORandomForestEstimator(max_depth = 40,\n",
    "    nfolds = 3, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n",
    "drf_model.train(X_cols, y_cols, train_h2o)\n",
    "pred = drf_model.predict(val_h2o[X_cols]).as_data_frame(use_pandas=True)\n",
    "mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred.as_matrix())\n",
    "np.sum(pred.as_matrix()<0)\n",
    "\n",
    "#18.126878064811716\n",
    "#18.115947532453333 max_depth = 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2 = H2OXGBoostEstimator(nfolds=3, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n",
    "model_d2.train(X_cols, y_cols, train_h2o)\n",
    "pred_d2 = model_d2.predict(val_h2o).as_data_frame(use_pandas=True)\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_d2.as_matrix()))\n",
    "# 20.443528867009622    None \n",
    "# 20.443176634720867   'sort_by_response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = H2OStackedEnsembleEstimator(model_id=\"my_ensemble\", training_frame=train_h2o, validation_frame=val_h2o,\n",
    "                                    base_models= [model_d2.model_id, drf_model.model_id])\n",
    "stack.train(X_cols, y_cols, train_h2o)\n",
    "pred_d3 = stack.predict(val_h2o).as_data_frame(use_pandas=True)\n",
    "print(mape_func(val_h2o[y_cols].as_data_frame().as_matrix(), pred_d3.as_matrix()))\n",
    "stack.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = test_h2o['ID'].as_data_frame()\n",
    "filename = \"/Users/mohsenkiskani/Downloads/Ubaar/submissions/submission76.csv\"\n",
    "with open(filename,\"w+\") as outputfile:\n",
    "    outputfile.write(\"ID,price\\n\")\n",
    "    for i in range(pred_df.shape[0]):\n",
    "        outputfile.write(str(tt.iloc[i])[6:17]+\",\"+str(int(np.ceil(pred_df.iloc[i])))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
