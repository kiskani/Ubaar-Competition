{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE          = 128\n",
    "TRAIN_EPOCHS        = 2000\n",
    "BIN_GRANULARITY     = 10\n",
    "HIDDEN_LAYER_1_SIZE = 512\n",
    "HIDDEN_LAYER_2_SIZE = 128 \n",
    "HIDDEN_LAYER_3_SIZE = 16\n",
    "lr                  = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(col):\n",
    "    return (col - np.mean(col)) / np.std(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the following link:\n",
    "https://www.kaggle.com/mmmarcy/tensorflow-dnn-regressor-with-feature-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data      = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/train.csv')\n",
    "test_data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/test.csv')\n",
    "\n",
    "data      = data.dropna(axis = 0)\n",
    "\n",
    "test_data.loc[12577, 'distanceKM']      = 52\n",
    "test_data.loc[12577, 'taxiDurationMin'] = 50\n",
    "test_data.loc[13853, 'distanceKM']      = 500\n",
    "test_data.loc[13853, 'taxiDurationMin'] = 380\n",
    "\n",
    "all_data = pd.concat((data, test_data)) \n",
    "\n",
    "ntrain = data.shape[0]\n",
    "ntest  = test_data.shape[0]\n",
    "\n",
    "categorical_vars = ['date', 'SourceState', 'destinationState', 'vehicleType', 'vehicleOption']\n",
    "continues_vars   = ['sourceLatitude', 'sourceLongitude', 'destinationLatitude', 'destinationLongitude',\n",
    "                    'distanceKM', 'taxiDurationMin', 'weight']\n",
    "\n",
    "all_data = all_data.copy()\n",
    "categorical_var_encoders = {}\n",
    "for var in categorical_vars:\n",
    "    le = preprocessing.LabelEncoder().fit(all_data[var])\n",
    "    all_data[var + '_ids']  = le.transform(all_data[var])\n",
    "    all_data[var + '_ids']  = all_data[var + '_ids'].astype('int32')\n",
    "    all_data.pop(var)\n",
    "    categorical_var_encoders[var] = le\n",
    "\n",
    "for cont in continues_vars:\n",
    "    all_data[cont] = all_data[cont].astype('float32')\n",
    "    \n",
    "all_data[\"distanceKM\"]      = normalize_column(all_data[\"distanceKM\"].values)\n",
    "all_data[\"taxiDurationMin\"] = normalize_column(all_data[\"taxiDurationMin\"].values)\n",
    "all_data[\"weight\"]          = normalize_column(all_data[\"weight\"].values)\n",
    "\n",
    "train    = all_data[:ntrain]\n",
    "test     = all_data[ntrain:]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, train['price'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_longitudes = set(all_data['sourceLongitude'].tolist() + all_data['destinationLongitude'].tolist())\n",
    "all_latitude   = set(all_data['sourceLatitude'].tolist() + all_data['destinationLatitude'].tolist())\n",
    "\n",
    "binned_long = np.linspace(min(all_longitudes), max(all_longitudes), BIN_GRANULARITY).tolist()\n",
    "binned_lat  = np.linspace(min(all_latitude), max(all_latitude), BIN_GRANULARITY).tolist()\n",
    "\n",
    "source_lat_feat         = tf.feature_column.numeric_column(\"sourceLatitude\") \n",
    "source_long_feat        = tf.feature_column.numeric_column(\"sourceLongitude\") \n",
    "destin_lat_feat         = tf.feature_column.numeric_column(\"destinationLatitude\") \n",
    "destin_long_feat        = tf.feature_column.numeric_column(\"destinationLongitude\") \n",
    "\n",
    "binned_source_lat_feat  = tf.feature_column.bucketized_column(\n",
    "                              source_column=source_lat_feat,\n",
    "                              boundaries= binned_lat)\n",
    "binned_source_long_feat = tf.feature_column.bucketized_column(\n",
    "                              source_column=source_long_feat,\n",
    "                              boundaries= binned_long)\n",
    "binned_destin_lat_feat  = tf.feature_column.bucketized_column(\n",
    "                              source_column=destin_lat_feat,\n",
    "                              boundaries= binned_lat)\n",
    "binned_destin_long_feat = tf.feature_column.bucketized_column(\n",
    "                              source_column=destin_long_feat,\n",
    "                              boundaries= binned_long)\n",
    "\n",
    "source_lat_x_long = tf.feature_column.embedding_column(tf.feature_column.crossed_column(\n",
    "                    keys=[binned_source_lat_feat, binned_source_long_feat], \n",
    "                    hash_bucket_size=BIN_GRANULARITY *BIN_GRANULARITY),dimension=BIN_GRANULARITY)\n",
    "\n",
    "destin_lat_x_long = tf.feature_column.embedding_column(tf.feature_column.crossed_column(\n",
    "                    keys=[binned_destin_lat_feat, binned_destin_long_feat], \n",
    "                    hash_bucket_size=BIN_GRANULARITY *BIN_GRANULARITY),dimension=BIN_GRANULARITY)\n",
    "\n",
    "distance_feat = tf.feature_column.numeric_column(\"distanceKM\")\n",
    "\n",
    "taximin_feat  = tf.feature_column.numeric_column(\"taxiDurationMin\")\n",
    "\n",
    "weight_feat   = tf.feature_column.numeric_column(\"weight\")\n",
    "\n",
    "date_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"date_ids\", 186),16)\n",
    "\n",
    "source_state_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"SourceState_ids\", 31),10)\n",
    "\n",
    "destin_state_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"destinationState_ids\", 31),10)\n",
    "\n",
    "veh_type_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"vehicleType_ids\", 4),4)\n",
    "\n",
    "veh_option_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"vehicleOption_ids\", 9),8)\n",
    "\n",
    "feature_columns = {source_lat_x_long, destin_lat_x_long, distance_feat, taximin_feat,\n",
    "                   weight_feat, date_feat, source_state_feat, destin_state_feat,\n",
    "                   veh_type_feat, veh_option_feat}\n",
    "\n",
    "#feature_columns = {distance_feat, taximin_feat, weight_feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(df, pred = False):\n",
    "    useful_fueatures = [\n",
    "        np.array(df[\"sourceLatitude\"].values, dtype=np.float32),\n",
    "        np.array(df[\"sourceLongitude\"].values, dtype=np.float32),\n",
    "        np.array(df[\"destinationLatitude\"].values, dtype=np.float32),\n",
    "        np.array(df[\"destinationLongitude\"].values, dtype=np.float32),\n",
    "        np.array(df[\"distanceKM\"].values, dtype=np.float32),\n",
    "        np.array(df[\"taxiDurationMin\"].values, dtype=np.float32),\n",
    "        np.array(df[\"weight\"].values, dtype=np.float32),\n",
    "        np.array(df[\"date_ids\"].values, dtype=np.int32),\n",
    "        np.array(df[\"SourceState_ids\"].values, dtype=np.int32),\n",
    "        np.array(df[\"destinationState_ids\"].values, dtype=np.int32),\n",
    "        np.array(df[\"vehicleType_ids\"].values, dtype=np.int32),\n",
    "        np.array(df[\"vehicleOption_ids\"].values, dtype=np.int32),\n",
    "        np.array(df[\"price\"].values, dtype=np.float32)\n",
    "    ]\n",
    "    \n",
    "    # Ugly, but creates all the slice input producers for all the features selected\n",
    "    sourceLatitude, sourceLongitude, destinationLatitude, destinationLongitude, distanceKM, taxiDurationMin,\\\n",
    "    weight, date_ids, SourceState_ids, destinationState_ids, vehicleType_ids, vehicleOption_ids, labels = \\\n",
    "    tf.train.slice_input_producer(\n",
    "        tensor_list=useful_fueatures,\n",
    "        num_epochs=TRAIN_EPOCHS,\n",
    "        shuffle=True,\n",
    "        capacity=BATCH_SIZE * 5\n",
    "    )\n",
    "\n",
    "    # Created a dict out of sliced input producers\n",
    "    dataset_dict = dict(\n",
    "        sourceLatitude=sourceLatitude,\n",
    "        sourceLongitude=sourceLongitude,\n",
    "        destinationLatitude=destinationLatitude,\n",
    "        destinationLongitude=destinationLongitude, \n",
    "        distanceKM=distanceKM,\n",
    "        taxiDurationMin=taxiDurationMin,\n",
    "        weight=weight,\n",
    "        date_ids=date_ids,\n",
    "        SourceState_ids=SourceState_ids,\n",
    "        destinationState_ids=destinationState_ids,\n",
    "        vehicleType_ids=vehicleType_ids,\n",
    "        vehicleOption_ids=vehicleOption_ids,\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "    # Creates a batched dictionary that holds a queue that loads the data\n",
    "    # while the training is happening. Multithreading.\n",
    "    batch_dict = tf.train.batch(\n",
    "        dataset_dict,\n",
    "        BATCH_SIZE,\n",
    "   )\n",
    "\n",
    "    if pred == False:\n",
    "        # The labels need to be returned separately\n",
    "        batch_labels = batch_dict.pop('labels')\n",
    "        return batch_dict, tf.reshape(batch_labels, [-1, 1])\n",
    "    else:\n",
    "        return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(features, labels, mode, params, config):\n",
    "    input_layer = tf.feature_column.input_layer(features=features, \n",
    "                                                feature_columns=feature_columns)\n",
    "    \n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    x = tf.layers.dense(inputs=input_layer,\n",
    "                        units=HIDDEN_LAYER_1_SIZE,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name=\"first_fully_connected_layer\")\n",
    "\n",
    "    x = tf.layers.dropout(inputs=x,name=\"first_dropout\")\n",
    "\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "                        units=HIDDEN_LAYER_2_SIZE,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name=\"second_fully_connected_layer\")\n",
    "\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "                        units=HIDDEN_LAYER_3_SIZE,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name=\"third_fully_connected_layer\")\n",
    "\n",
    "    predictions = tf.contrib.layers.fully_connected(inputs=x, num_outputs=1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT :\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss  = tf.reduce_mean(tf.abs(tf.divide(predictions-labels,labels))) \n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions,\n",
    "                                          loss=loss)\n",
    "    else:\n",
    "        #loss = tf.losses.absolute_difference(labels=labels,\n",
    "        #                                    predictions=predictions)\n",
    "        loss  = tf.reduce_mean(tf.abs(tf.divide(predictions-labels,labels))) \n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "        train_op = optimizer.minimize(loss, \n",
    "                                      global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, \n",
    "                                          predictions=predictions,\n",
    "                                          loss=loss, \n",
    "                                          train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpbv8jaice\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpbv8jaice', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c20d614a8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpbv8jaice/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 49.0603\n",
      "INFO:tensorflow:loss = 0.25560293, step = 101 (2.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3206\n",
      "INFO:tensorflow:loss = 0.24707279, step = 201 (1.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5059\n",
      "INFO:tensorflow:loss = 0.27837402, step = 301 (1.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.592\n",
      "INFO:tensorflow:loss = 0.22700217, step = 401 (1.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.59\n",
      "INFO:tensorflow:loss = 0.27893358, step = 501 (1.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3382\n",
      "INFO:tensorflow:loss = 0.21584475, step = 601 (1.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3024\n",
      "INFO:tensorflow:loss = 0.25908747, step = 701 (1.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4541\n",
      "INFO:tensorflow:loss = 0.2738995, step = 801 (1.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4731\n",
      "INFO:tensorflow:loss = 0.256431, step = 901 (1.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5318\n",
      "INFO:tensorflow:loss = 0.2533884, step = 1001 (1.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5207\n",
      "INFO:tensorflow:loss = 0.20463458, step = 1101 (1.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4062\n",
      "INFO:tensorflow:loss = 0.20870851, step = 1201 (1.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3164\n",
      "INFO:tensorflow:loss = 0.21902007, step = 1301 (1.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.6401\n",
      "INFO:tensorflow:loss = 0.23105966, step = 1401 (1.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4249\n",
      "INFO:tensorflow:loss = 0.21972576, step = 1501 (1.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5806\n",
      "INFO:tensorflow:loss = 0.19333003, step = 1601 (1.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5176\n",
      "INFO:tensorflow:loss = 0.23237202, step = 1701 (1.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4948\n",
      "INFO:tensorflow:loss = 0.2175799, step = 1801 (1.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.6983\n",
      "INFO:tensorflow:loss = 0.21242583, step = 1901 (1.733 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpbv8jaice/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21705347.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1a1862f128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(learning_rate=lr)\n",
    "estimator = tf.estimator.Estimator(model_fn=make_model,\n",
    "                                   params=hparams)\n",
    "estimator.train(input_fn=lambda: input_fn(X_train),\n",
    "                steps=TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-05-05:02:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpbv8jaice/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-05-05:02:00\n",
      "INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, loss = 0.24969374\n"
     ]
    }
   ],
   "source": [
    "ev = estimator.evaluate(input_fn=lambda: input_fn(X_val), steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpbv8jaice/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "y = estimator.predict(input_fn=lambda: input_fn(X_val))\n",
    "predictions = list(itertools.islice(y, X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(predictions)):\n",
    "    y_pred += predictions[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.85937874782954"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = mean_absolute_percentage_error(y_val, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn_train = tf.estimator.inputs.pandas_input_fn(x=X_train.drop(['ID','price'], axis=1),\n",
    "                                                     y=X_train['price'].astype('float32'),\n",
    "                                                     num_epochs=TRAIN_EPOCHS,\n",
    "                                                     batch_size=128,\n",
    "                                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpaf6ggs3x\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpaf6ggs3x', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a15c009e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpaf6ggs3x/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 98.3095\n",
      "INFO:tensorflow:loss = 0.5270034, step = 101 (1.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.162\n",
      "INFO:tensorflow:loss = 0.55100286, step = 201 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.875\n",
      "INFO:tensorflow:loss = 0.5815655, step = 301 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.803\n",
      "INFO:tensorflow:loss = 0.62595934, step = 401 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.755\n",
      "INFO:tensorflow:loss = 0.57237417, step = 501 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.922\n",
      "INFO:tensorflow:loss = 0.5974933, step = 601 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.008\n",
      "INFO:tensorflow:loss = 0.57348335, step = 701 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.984\n",
      "INFO:tensorflow:loss = 0.5094887, step = 801 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.221\n",
      "INFO:tensorflow:loss = 0.5350514, step = 901 (0.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.369\n",
      "INFO:tensorflow:loss = 0.5484915, step = 1001 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.717\n",
      "INFO:tensorflow:loss = 0.48790458, step = 1101 (0.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.225\n",
      "INFO:tensorflow:loss = 0.55944645, step = 1201 (0.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.51\n",
      "INFO:tensorflow:loss = 0.5228667, step = 1301 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.347\n",
      "INFO:tensorflow:loss = 0.63274336, step = 1401 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.584\n",
      "INFO:tensorflow:loss = 0.49012274, step = 1501 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.727\n",
      "INFO:tensorflow:loss = 0.553828, step = 1601 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.192\n",
      "INFO:tensorflow:loss = 0.5732745, step = 1701 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.288\n",
      "INFO:tensorflow:loss = 0.58051026, step = 1801 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.771\n",
      "INFO:tensorflow:loss = 0.53771484, step = 1901 (0.593 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmpaf6ggs3x/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.54721916.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1a15c00470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(learning_rate=lr)\n",
    "estimator = tf.estimator.Estimator(model_fn=make_model,params=hparams) \n",
    "estimator.train(input_fn=input_fn_train,steps=TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"/Users/mohsenkiskani/Downloads/Ubaar/submissions/submission14.csv\"\n",
    "#with open(filename,\"w+\") as outputfile:\n",
    "#    outputfile.write(\"ID,price\\n\")\n",
    "#    for i in range(len(y_pred_test)):\n",
    "#        outputfile.write(str(test_data.ID[i])+\",\"+str(int(np.ceil(y_pred_test[i])))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
