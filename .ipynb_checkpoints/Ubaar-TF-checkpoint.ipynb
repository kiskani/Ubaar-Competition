{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE          = 128\n",
    "TRAIN_EPOCHS        = 7000\n",
    "BIN_GRANULARITY     = 100\n",
    "HIDDEN_LAYER_1_SIZE = 512\n",
    "HIDDEN_LAYER_2_SIZE = 128 \n",
    "HIDDEN_LAYER_3_SIZE = 16\n",
    "lr                  = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "def get_score(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def scale_minmax(col):\n",
    "    return (col-col.min())/(col.max()-col.min())\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the following link:\n",
    "https://www.kaggle.com/mmmarcy/tensorflow-dnn-regressor-with-feature-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>destinationLatitude</th>\n",
       "      <th>destinationLongitude</th>\n",
       "      <th>distanceKM</th>\n",
       "      <th>price</th>\n",
       "      <th>sourceLatitude</th>\n",
       "      <th>sourceLongitude</th>\n",
       "      <th>taxiDurationMin</th>\n",
       "      <th>weight</th>\n",
       "      <th>date_ids</th>\n",
       "      <th>SourceState_ids</th>\n",
       "      <th>destinationState_ids</th>\n",
       "      <th>vehicleType_ids</th>\n",
       "      <th>vehicleOption_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23990</th>\n",
       "      <td>18426018643</td>\n",
       "      <td>32.669956</td>\n",
       "      <td>51.670528</td>\n",
       "      <td>1.399091</td>\n",
       "      <td>12500000.0</td>\n",
       "      <td>39.344208</td>\n",
       "      <td>45.064754</td>\n",
       "      <td>1.374359</td>\n",
       "      <td>-0.659986</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>68800274778</td>\n",
       "      <td>31.318350</td>\n",
       "      <td>48.681923</td>\n",
       "      <td>-0.244049</td>\n",
       "      <td>9000000.0</td>\n",
       "      <td>32.325771</td>\n",
       "      <td>50.847469</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>1.541320</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>80477779395</td>\n",
       "      <td>38.550434</td>\n",
       "      <td>44.953541</td>\n",
       "      <td>-1.415570</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>38.550434</td>\n",
       "      <td>44.953541</td>\n",
       "      <td>-1.515664</td>\n",
       "      <td>-1.048451</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>28328675917</td>\n",
       "      <td>28.967031</td>\n",
       "      <td>50.843033</td>\n",
       "      <td>-1.001649</td>\n",
       "      <td>1775000.0</td>\n",
       "      <td>29.614637</td>\n",
       "      <td>51.654930</td>\n",
       "      <td>-0.995007</td>\n",
       "      <td>-0.012543</td>\n",
       "      <td>135</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38351</th>\n",
       "      <td>14704494197</td>\n",
       "      <td>34.092155</td>\n",
       "      <td>49.693226</td>\n",
       "      <td>-0.713159</td>\n",
       "      <td>5880000.0</td>\n",
       "      <td>35.693634</td>\n",
       "      <td>51.407555</td>\n",
       "      <td>-0.757316</td>\n",
       "      <td>1.023366</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  destinationLatitude  destinationLongitude  distanceKM  \\\n",
       "23990  18426018643            32.669956             51.670528    1.399091   \n",
       "8729   68800274778            31.318350             48.681923   -0.244049   \n",
       "3451   80477779395            38.550434             44.953541   -1.415570   \n",
       "2628   28328675917            28.967031             50.843033   -1.001649   \n",
       "38351  14704494197            34.092155             49.693226   -0.713159   \n",
       "\n",
       "            price  sourceLatitude  sourceLongitude  taxiDurationMin    weight  \\\n",
       "23990  12500000.0       39.344208        45.064754         1.374359 -0.659986   \n",
       "8729    9000000.0       32.325771        50.847469         0.008578  1.541320   \n",
       "3451    1000000.0       38.550434        44.953541        -1.515664 -1.048451   \n",
       "2628    1775000.0       29.614637        51.654930        -0.995007 -0.012543   \n",
       "38351   5880000.0       35.693634        51.407555        -0.757316  1.023366   \n",
       "\n",
       "       date_ids  SourceState_ids  destinationState_ids  vehicleType_ids  \\\n",
       "23990        88                1                     3                3   \n",
       "8729         17               23                    11                3   \n",
       "3451        178                1                     1                1   \n",
       "2628        135               15                     6                2   \n",
       "38351        22                7                    20                3   \n",
       "\n",
       "       vehicleOption_ids  \n",
       "23990                  7  \n",
       "8729                   4  \n",
       "3451                   5  \n",
       "2628                   3  \n",
       "38351                  7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data      = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/train.csv')\n",
    "test_data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/test.csv')\n",
    "\n",
    "data      = data.dropna(axis = 0)\n",
    "\n",
    "test_data.loc[12577, 'distanceKM']      = 52\n",
    "test_data.loc[12577, 'taxiDurationMin'] = 50\n",
    "test_data.loc[13853, 'distanceKM']      = 500\n",
    "test_data.loc[13853, 'taxiDurationMin'] = 380\n",
    "\n",
    "all_data = pd.concat((data, test_data)) \n",
    "\n",
    "min_price = min(all_data['price'])\n",
    "\n",
    "ntrain = data.shape[0]\n",
    "ntest  = test_data.shape[0]\n",
    "\n",
    "categorical_vars = ['date', 'SourceState', 'destinationState', 'vehicleType', 'vehicleOption']\n",
    "continues_vars   = ['sourceLatitude', 'sourceLongitude', 'destinationLatitude', 'destinationLongitude',\n",
    "                    'distanceKM', 'taxiDurationMin', 'weight']\n",
    "\n",
    "all_data = all_data.copy()\n",
    "categorical_var_encoders = {}\n",
    "for var in categorical_vars:\n",
    "    le = preprocessing.LabelEncoder().fit(all_data[var])\n",
    "    all_data[var + '_ids']  = le.transform(all_data[var])\n",
    "    all_data[var + '_ids']  = all_data[var + '_ids'].astype('int32')\n",
    "    all_data.pop(var)\n",
    "    categorical_var_encoders[var] = le\n",
    "\n",
    "for cont in continues_vars:\n",
    "    all_data[cont] = all_data[cont].astype('float32')\n",
    "    \n",
    "all_data[\"distanceKM\"]      = normalize_column(all_data[\"distanceKM\"].values)\n",
    "all_data[\"taxiDurationMin\"] = normalize_column(all_data[\"taxiDurationMin\"].values)\n",
    "all_data[\"weight\"]          = normalize_column(all_data[\"weight\"].values)\n",
    "\n",
    "train    = all_data[:ntrain]\n",
    "test     = all_data[ntrain:]\n",
    "\n",
    "train['price']  = train['price'].astype('float32')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, train['price'], test_size=0.33, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_longitudes = set(all_data['sourceLongitude'].tolist() + all_data['destinationLongitude'].tolist())\n",
    "all_latitude   = set(all_data['sourceLatitude'].tolist() + all_data['destinationLatitude'].tolist())\n",
    "\n",
    "binned_long = np.linspace(min(all_longitudes), max(all_longitudes), BIN_GRANULARITY).tolist()\n",
    "binned_lat  = np.linspace(min(all_latitude), max(all_latitude), BIN_GRANULARITY).tolist()\n",
    "\n",
    "source_lat_feat         = tf.feature_column.numeric_column(\"sourceLatitude\") \n",
    "source_long_feat        = tf.feature_column.numeric_column(\"sourceLongitude\") \n",
    "destin_lat_feat         = tf.feature_column.numeric_column(\"destinationLatitude\") \n",
    "destin_long_feat        = tf.feature_column.numeric_column(\"destinationLongitude\") \n",
    "\n",
    "binned_source_lat_feat  = tf.feature_column.bucketized_column(\n",
    "                              source_column=source_lat_feat,\n",
    "                              boundaries= binned_lat)\n",
    "binned_source_long_feat = tf.feature_column.bucketized_column(\n",
    "                              source_column=source_long_feat,\n",
    "                              boundaries= binned_long)\n",
    "binned_destin_lat_feat  = tf.feature_column.bucketized_column(\n",
    "                              source_column=destin_lat_feat,\n",
    "                              boundaries= binned_lat)\n",
    "binned_destin_long_feat = tf.feature_column.bucketized_column(\n",
    "                              source_column=destin_long_feat,\n",
    "                              boundaries= binned_long)\n",
    "\n",
    "source_lat_x_long = tf.feature_column.embedding_column(tf.feature_column.crossed_column(\n",
    "                    keys=[binned_source_lat_feat, binned_source_long_feat], \n",
    "                    hash_bucket_size=BIN_GRANULARITY *BIN_GRANULARITY),dimension=BIN_GRANULARITY)\n",
    "\n",
    "destin_lat_x_long = tf.feature_column.embedding_column(tf.feature_column.crossed_column(\n",
    "                    keys=[binned_destin_lat_feat, binned_destin_long_feat], \n",
    "                    hash_bucket_size=BIN_GRANULARITY *BIN_GRANULARITY),dimension=BIN_GRANULARITY)\n",
    "\n",
    "distance_feat = tf.feature_column.numeric_column(\"distanceKM\")\n",
    "\n",
    "taximin_feat  = tf.feature_column.numeric_column(\"taxiDurationMin\")\n",
    "\n",
    "weight_feat   = tf.feature_column.numeric_column(\"weight\")\n",
    "\n",
    "date_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"date_ids\", 186),16)\n",
    "\n",
    "source_state_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"SourceState_ids\", 31),10)\n",
    "\n",
    "destin_state_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"destinationState_ids\", 31),10)\n",
    "\n",
    "veh_type_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"vehicleType_ids\", 4),4)\n",
    "\n",
    "veh_option_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\"vehicleOption_ids\", 9),8)\n",
    "\n",
    "feature_columns = {source_lat_x_long, destin_lat_x_long, distance_feat, taximin_feat,\n",
    "                   weight_feat, date_feat, source_state_feat, destin_state_feat,\n",
    "                   veh_type_feat, veh_option_feat}\n",
    "\n",
    "#feature_columns = {distance_feat, taximin_feat, weight_feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(df, pred = False):\n",
    "    \n",
    "    #useful_fueatures = [\n",
    "    #    np.array(df[\"sourceLatitude\"].values, dtype=np.float32),\n",
    "    #    np.array(df[\"sourceLongitude\"].values, dtype=np.float32),\n",
    "    #    np.array(df[\"destinationLatitude\"].values, dtype=np.float32),\n",
    "    #    np.array(df[\"destinationLongitude\"].values, dtype=np.float32),\n",
    "    #    np.array(df[\"distanceKM\"].values, dtype=np.float32),\n",
    "    #    np.array(df[\"taxiDurationMin\"].values, dtype=np.float32),\n",
    "    #    np.array(df[\"weight\"].values, dtype=np.float32),\n",
    "    #    np.array(df[\"date_ids\"].values, dtype=np.int32),\n",
    "    #    np.array(df[\"SourceState_ids\"].values, dtype=np.int32),\n",
    "    #    np.array(df[\"destinationState_ids\"].values, dtype=np.int32),\n",
    "    #    np.array(df[\"vehicleType_ids\"].values, dtype=np.int32),\n",
    "    #    np.array(df[\"vehicleOption_ids\"].values, dtype=np.int32),\n",
    "    #    np.array(df[\"ID\"].values, dtype=np.int32),\n",
    "    #    np.array(df[\"price\"].values, dtype=np.float32)\n",
    "    #]\n",
    "    \n",
    "    useful_fueatures = [\n",
    "        np.array(df[\"sourceLatitude\"]),\n",
    "        np.array(df[\"sourceLongitude\"]),\n",
    "        np.array(df[\"destinationLatitude\"]),\n",
    "        np.array(df[\"destinationLongitude\"]),\n",
    "        np.array(df[\"distanceKM\"]),\n",
    "        np.array(df[\"taxiDurationMin\"]),\n",
    "        np.array(df[\"weight\"]),\n",
    "        np.array(df[\"date_ids\"]),\n",
    "        np.array(df[\"SourceState_ids\"]),\n",
    "        np.array(df[\"destinationState_ids\"]),\n",
    "        np.array(df[\"vehicleType_ids\"]),\n",
    "        np.array(df[\"vehicleOption_ids\"]),\n",
    "        np.array(df[\"ID\"]),\n",
    "        np.array(df[\"price\"])\n",
    "    ]\n",
    "    \n",
    "    # Ugly, but creates all the slice input producers for all the features selected\n",
    "    sourceLatitude, sourceLongitude, destinationLatitude, destinationLongitude, distanceKM, taxiDurationMin,\\\n",
    "    weight, date_ids, SourceState_ids, destinationState_ids, vehicleType_ids, vehicleOption_ids, ID, labels = \\\n",
    "    tf.train.slice_input_producer(\n",
    "        tensor_list=useful_fueatures,\n",
    "        num_epochs=TRAIN_EPOCHS,\n",
    "        shuffle=True,\n",
    "        capacity=BATCH_SIZE * 5\n",
    "    )\n",
    "\n",
    "    # Created a dict out of sliced input producers\n",
    "    dataset_dict = dict(\n",
    "        sourceLatitude=sourceLatitude,\n",
    "        sourceLongitude=sourceLongitude,\n",
    "        destinationLatitude=destinationLatitude,\n",
    "        destinationLongitude=destinationLongitude, \n",
    "        distanceKM=distanceKM,\n",
    "        taxiDurationMin=taxiDurationMin,\n",
    "        weight=weight,\n",
    "        date_ids=date_ids,\n",
    "        SourceState_ids=SourceState_ids,\n",
    "        destinationState_ids=destinationState_ids,\n",
    "        vehicleType_ids=vehicleType_ids,\n",
    "        vehicleOption_ids=vehicleOption_ids,\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "    # Creates a batched dictionary that holds a queue that loads the data\n",
    "    # while the training is happening. Multithreading.\n",
    "    batch_dict = tf.train.batch(\n",
    "        dataset_dict,\n",
    "        BATCH_SIZE,\n",
    "   )\n",
    "\n",
    "    if pred == False:\n",
    "        # The labels need to be returned separately\n",
    "        batch_labels = batch_dict.pop('labels')\n",
    "        return batch_dict, tf.reshape(batch_labels, [-1, 1]) \n",
    "    else:\n",
    "        return batch_dict#, ID #{ID : batch_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(features, labels, mode, params, config):\n",
    "    input_layer = tf.feature_column.input_layer(features=features, \n",
    "                                                feature_columns=feature_columns)\n",
    "    \n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    x = tf.layers.dense(inputs=input_layer,\n",
    "                        units=HIDDEN_LAYER_1_SIZE,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name=\"first_fully_connected_layer\")\n",
    "\n",
    "    x = tf.layers.dropout(inputs=x,name=\"first_dropout\")\n",
    "\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "                        units=HIDDEN_LAYER_2_SIZE,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name=\"second_fully_connected_layer\")\n",
    "\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "                        units=HIDDEN_LAYER_3_SIZE,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name=\"third_fully_connected_layer\")\n",
    "\n",
    "    predictions = tf.contrib.layers.fully_connected(inputs=x, num_outputs=1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT :\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss  = tf.reduce_mean(tf.abs(tf.divide(predictions-labels,labels))) \n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions,\n",
    "                                          loss=loss)\n",
    "    else:\n",
    "        #loss = tf.losses.absolute_difference(labels=labels,\n",
    "        #                                    predictions=predictions)\n",
    "        loss  = tf.reduce_mean(tf.abs(tf.divide(predictions-labels,labels))) \n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "        train_op = optimizer.minimize(loss, \n",
    "                                      global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, \n",
    "                                          predictions=predictions,\n",
    "                                          loss=loss, \n",
    "                                          train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmp9we_mch2\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmp9we_mch2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c288019e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmp9we_mch2/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 36.1895\n",
      "INFO:tensorflow:loss = 0.9999987, step = 101 (2.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.1039\n",
      "INFO:tensorflow:loss = 0.99937284, step = 201 (2.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.6317\n",
      "INFO:tensorflow:loss = 0.673558, step = 301 (2.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0418\n",
      "INFO:tensorflow:loss = 0.56163895, step = 401 (2.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.1603\n",
      "INFO:tensorflow:loss = 0.47735244, step = 501 (2.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.1495\n",
      "INFO:tensorflow:loss = 0.45720166, step = 601 (2.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.606\n",
      "INFO:tensorflow:loss = 0.41347623, step = 701 (2.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4543\n",
      "INFO:tensorflow:loss = 0.38834023, step = 801 (2.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.715\n",
      "INFO:tensorflow:loss = 0.40356416, step = 901 (2.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9657\n",
      "INFO:tensorflow:loss = 0.37171477, step = 1001 (2.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3068\n",
      "INFO:tensorflow:loss = 0.32940808, step = 1101 (2.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6757\n",
      "INFO:tensorflow:loss = 0.34848082, step = 1201 (2.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0417\n",
      "INFO:tensorflow:loss = 0.27159923, step = 1301 (2.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5749\n",
      "INFO:tensorflow:loss = 0.29747078, step = 1401 (2.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4815\n",
      "INFO:tensorflow:loss = 0.28693566, step = 1501 (2.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.2349\n",
      "INFO:tensorflow:loss = 0.27395344, step = 1601 (2.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9108\n",
      "INFO:tensorflow:loss = 0.25199422, step = 1701 (2.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9083\n",
      "INFO:tensorflow:loss = 0.24420822, step = 1801 (2.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6856\n",
      "INFO:tensorflow:loss = 0.22610486, step = 1901 (2.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8315\n",
      "INFO:tensorflow:loss = 0.23039392, step = 2001 (2.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6655\n",
      "INFO:tensorflow:loss = 0.1997053, step = 2101 (2.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8305\n",
      "INFO:tensorflow:loss = 0.20409675, step = 2201 (2.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9494\n",
      "INFO:tensorflow:loss = 0.2255556, step = 2301 (2.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8503\n",
      "INFO:tensorflow:loss = 0.17957434, step = 2401 (2.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8144\n",
      "INFO:tensorflow:loss = 0.22025965, step = 2501 (2.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7903\n",
      "INFO:tensorflow:loss = 0.23501018, step = 2601 (2.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6882\n",
      "INFO:tensorflow:loss = 0.24428412, step = 2701 (2.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0728\n",
      "INFO:tensorflow:loss = 0.20050095, step = 2801 (2.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6761\n",
      "INFO:tensorflow:loss = 0.20719963, step = 2901 (2.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6152\n",
      "INFO:tensorflow:loss = 0.20259923, step = 3001 (2.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6388\n",
      "INFO:tensorflow:loss = 0.22017378, step = 3101 (2.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6469\n",
      "INFO:tensorflow:loss = 0.20478535, step = 3201 (2.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8941\n",
      "INFO:tensorflow:loss = 0.2525645, step = 3301 (2.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8478\n",
      "INFO:tensorflow:loss = 0.20571212, step = 3401 (2.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5589\n",
      "INFO:tensorflow:loss = 0.20170482, step = 3501 (2.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7187\n",
      "INFO:tensorflow:loss = 0.20267312, step = 3601 (2.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9257\n",
      "INFO:tensorflow:loss = 0.21630791, step = 3701 (2.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.895\n",
      "INFO:tensorflow:loss = 0.19025332, step = 3801 (2.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9712\n",
      "INFO:tensorflow:loss = 0.2224144, step = 3901 (2.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4461\n",
      "INFO:tensorflow:loss = 0.1956287, step = 4001 (2.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7318\n",
      "INFO:tensorflow:loss = 0.18044195, step = 4101 (2.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8448\n",
      "INFO:tensorflow:loss = 0.22669029, step = 4201 (2.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.647\n",
      "INFO:tensorflow:loss = 0.17829521, step = 4301 (2.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9513\n",
      "INFO:tensorflow:loss = 0.19436568, step = 4401 (2.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.1367\n",
      "INFO:tensorflow:loss = 0.18167841, step = 4501 (2.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.2572\n",
      "INFO:tensorflow:loss = 0.20077187, step = 4601 (2.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5492\n",
      "INFO:tensorflow:loss = 0.2017532, step = 4701 (2.060 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.725\n",
      "INFO:tensorflow:loss = 0.22488257, step = 4801 (2.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8588\n",
      "INFO:tensorflow:loss = 0.15993753, step = 4901 (2.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0726\n",
      "INFO:tensorflow:loss = 0.17691512, step = 5001 (2.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9108\n",
      "INFO:tensorflow:loss = 0.2085987, step = 5101 (2.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6985\n",
      "INFO:tensorflow:loss = 0.2009249, step = 5201 (2.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5227\n",
      "INFO:tensorflow:loss = 0.21125913, step = 5301 (2.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9031\n",
      "INFO:tensorflow:loss = 0.18885782, step = 5401 (2.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4943\n",
      "INFO:tensorflow:loss = 0.16502693, step = 5501 (2.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0116\n",
      "INFO:tensorflow:loss = 0.20023659, step = 5601 (2.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9539\n",
      "INFO:tensorflow:loss = 0.20233607, step = 5701 (2.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7733\n",
      "INFO:tensorflow:loss = 0.18475229, step = 5801 (2.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6709\n",
      "INFO:tensorflow:loss = 0.1791771, step = 5901 (2.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0199\n",
      "INFO:tensorflow:loss = 0.21423374, step = 6001 (2.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.7572\n",
      "INFO:tensorflow:loss = 0.17574862, step = 6101 (2.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.1522\n",
      "INFO:tensorflow:loss = 0.16368477, step = 6201 (2.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7735\n",
      "INFO:tensorflow:loss = 0.16192135, step = 6301 (2.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.9114\n",
      "INFO:tensorflow:loss = 0.1796656, step = 6401 (2.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.4209\n",
      "INFO:tensorflow:loss = 0.16532804, step = 6501 (2.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5384\n",
      "INFO:tensorflow:loss = 0.21542639, step = 6601 (2.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.8282\n",
      "INFO:tensorflow:loss = 0.16412099, step = 6701 (2.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1544\n",
      "INFO:tensorflow:loss = 0.17687958, step = 6801 (2.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1417\n",
      "INFO:tensorflow:loss = 0.17975894, step = 6901 (2.318 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7000 into /var/folders/hy/j_c72d1x72g_rr58tgrlh3b40000gn/T/tmp9we_mch2/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.17827721.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1a1d27da20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(learning_rate=lr)\n",
    "estimator = tf.estimator.Estimator(model_fn=make_model, params=hparams)\n",
    "estimator.train(input_fn=lambda: input_fn(train), steps=TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = estimator.evaluate(input_fn=lambda: input_fn(X_val), steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_treu = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "    start_time = time.time()\n",
    "    y_treu.append(int(y_val.iloc[i]))\n",
    "    X_val2 = X_val.iloc[i:i+1]\n",
    "    rt     = estimator.predict(input_fn=lambda: input_fn(X_val2, pred=True))\n",
    "    pred   = list(itertools.islice(rt, X_val2.shape[0]))\n",
    "    y_pred.append(int(pred[0]))\n",
    "    print(i, time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mean_absolute_percentage_error(np.array(y_treu[:-1]), np.array(y_pred))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test = {}\n",
    "\n",
    "#for i in range(test.shape[0]):\n",
    "#    start_time = time.time()\n",
    "#    X_test2    = test.iloc[i:i+1]\n",
    "#    rt         = estimator.predict(input_fn=lambda: input_fn(X_test2, pred=True))\n",
    "#    pred       = list(itertools.islice(rt, X_test2.shape[0]))\n",
    "#    y_test[int(X_test2['ID'])]   = int(pred[0])\n",
    "#    print(i, time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_fn_train = tf.estimator.inputs.pandas_input_fn(x=X_train.drop(['ID','price'], axis=1),\n",
    "#                                                     y=X_train['price'].astype('float32'),\n",
    "#                                                     num_epochs=TRAIN_EPOCHS,\n",
    "#                                                     batch_size=128,\n",
    "#                                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hparams = tf.contrib.training.HParams(learning_rate=lr)\n",
    "#estimator = tf.estimator.Estimator(model_fn=make_model,params=hparams) \n",
    "#estimator.train(input_fn=input_fn_train,steps=TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"/Users/mohsenkiskani/Downloads/Ubaar/submissions/submission14.csv\"\n",
    "#with open(filename,\"w+\") as outputfile:\n",
    "#    outputfile.write(\"ID,price\\n\")\n",
    "#    for i in range(len(y_pred_test)):\n",
    "#        outputfile.write(str(test_data.ID[i])+\",\"+str(int(np.ceil(y_pred_test[i])))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
