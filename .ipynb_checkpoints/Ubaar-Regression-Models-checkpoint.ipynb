{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "def get_score(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def scale_minmax(col):\n",
    "    return (col-col.min())/(col.max()-col.min())\n",
    "\n",
    "def mean_absolute_precision_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destinationLatitude</th>\n",
       "      <th>destinationLongitude</th>\n",
       "      <th>distanceKM</th>\n",
       "      <th>sourceLatitude</th>\n",
       "      <th>sourceLongitude</th>\n",
       "      <th>taxiDurationMin</th>\n",
       "      <th>weight</th>\n",
       "      <th>date_ids</th>\n",
       "      <th>SourceState_ids</th>\n",
       "      <th>destinationState_ids</th>\n",
       "      <th>vehicleType_ids</th>\n",
       "      <th>vehicleOption_ids</th>\n",
       "      <th>source_tuple_ids</th>\n",
       "      <th>destination_tuple_ids</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39085</th>\n",
       "      <td>36.473089</td>\n",
       "      <td>52.349822</td>\n",
       "      <td>184.0</td>\n",
       "      <td>35.700109</td>\n",
       "      <td>51.399743</td>\n",
       "      <td>199.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>124</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1410</td>\n",
       "      <td>1774</td>\n",
       "      <td>1834.976428</td>\n",
       "      <td>1909.359717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30892</th>\n",
       "      <td>35.704176</td>\n",
       "      <td>51.400280</td>\n",
       "      <td>331.0</td>\n",
       "      <td>37.275731</td>\n",
       "      <td>49.584392</td>\n",
       "      <td>254.0</td>\n",
       "      <td>1.67</td>\n",
       "      <td>118</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1926</td>\n",
       "      <td>1515</td>\n",
       "      <td>1848.294458</td>\n",
       "      <td>1835.204644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45277</th>\n",
       "      <td>35.699924</td>\n",
       "      <td>51.396715</td>\n",
       "      <td>447.0</td>\n",
       "      <td>32.665899</td>\n",
       "      <td>51.663805</td>\n",
       "      <td>285.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>801</td>\n",
       "      <td>1515</td>\n",
       "      <td>1687.644636</td>\n",
       "      <td>1834.858819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>30.199563</td>\n",
       "      <td>53.182966</td>\n",
       "      <td>809.0</td>\n",
       "      <td>35.699078</td>\n",
       "      <td>51.401589</td>\n",
       "      <td>525.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1410</td>\n",
       "      <td>463</td>\n",
       "      <td>1834.989335</td>\n",
       "      <td>1606.102332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>27.180941</td>\n",
       "      <td>56.277756</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>34.643252</td>\n",
       "      <td>50.877469</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1176</td>\n",
       "      <td>72</td>\n",
       "      <td>1762.560980</td>\n",
       "      <td>1529.682365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       destinationLatitude  destinationLongitude  distanceKM  sourceLatitude  \\\n",
       "39085            36.473089             52.349822       184.0       35.700109   \n",
       "30892            35.704176             51.400280       331.0       37.275731   \n",
       "45277            35.699924             51.396715       447.0       32.665899   \n",
       "16398            30.199563             53.182966       809.0       35.699078   \n",
       "13653            27.180941             56.277756      1144.0       34.643252   \n",
       "\n",
       "       sourceLongitude  taxiDurationMin  weight  date_ids  SourceState_ids  \\\n",
       "39085        51.399743            199.0   21.00       124                7   \n",
       "30892        49.584392            254.0    1.67       118               29   \n",
       "45277        51.663805            285.0   19.00        83                3   \n",
       "16398        51.401589            525.0    4.00       151                7   \n",
       "13653        50.877469            750.0    2.00        85               17   \n",
       "\n",
       "       destinationState_ids  vehicleType_ids  vehicleOption_ids  \\\n",
       "39085                    19                3                  2   \n",
       "30892                     7                1                  5   \n",
       "45277                     7                3                  7   \n",
       "16398                    15                1                  5   \n",
       "13653                    21                1                  5   \n",
       "\n",
       "       source_tuple_ids  destination_tuple_ids       source  destination  \n",
       "39085              1410                   1774  1834.976428  1909.359717  \n",
       "30892              1926                   1515  1848.294458  1835.204644  \n",
       "45277               801                   1515  1687.644636  1834.858819  \n",
       "16398              1410                    463  1834.989335  1606.102332  \n",
       "13653              1176                     72  1762.560980  1529.682365  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data      = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/train.csv')\n",
    "test_data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/test.csv')\n",
    "\n",
    "data      = data.dropna(axis = 0)\n",
    "\n",
    "test_data.loc[12577, 'distanceKM']      = 52\n",
    "test_data.loc[12577, 'taxiDurationMin'] = 50\n",
    "test_data.loc[13853, 'distanceKM']      = 500\n",
    "test_data.loc[13853, 'taxiDurationMin'] = 380\n",
    "\n",
    "all_data = pd.concat((data, test_data)) \n",
    "\n",
    "min_price = min(all_data['price'])\n",
    "\n",
    "ntrain = data.shape[0]\n",
    "ntest  = test_data.shape[0]\n",
    "\n",
    "BUCKET_LATI = 1000\n",
    "BUCKET_LONG = 1000\n",
    "\n",
    "min_source_lat  = min(all_data['sourceLatitude'])\n",
    "min_destin_lat  = min(all_data['destinationLatitude'])\n",
    "min_lat         = min(min_destin_lat, min_source_lat)\n",
    "\n",
    "min_source_long = min(all_data['sourceLongitude'])\n",
    "min_destin_long = min(all_data['destinationLongitude'])\n",
    "min_long        = min(min_destin_long, min_source_long)\n",
    "\n",
    "max_source_lat  = max(all_data['sourceLatitude'])\n",
    "max_destin_lat  = max(all_data['destinationLatitude'])\n",
    "max_lat         = max(max_destin_lat, max_source_lat)\n",
    "\n",
    "max_source_long = max(all_data['sourceLongitude'])\n",
    "max_destin_long = max(all_data['destinationLongitude'])\n",
    "max_long        = max(max_destin_long, max_source_long)\n",
    "\n",
    "d_lati = (max_lat - min_lat)/BUCKET_LATI\n",
    "d_long = (max_long - min_long)/BUCKET_LONG\n",
    "\n",
    "destin_lati_bucket = (all_data['destinationLatitude']  // d_lati).as_matrix().astype(int)\n",
    "destin_long_bucket = (all_data['destinationLongitude'] // d_long).as_matrix().astype(int)\n",
    "\n",
    "all_data['destination_tuple'] = tuple(zip(destin_lati_bucket,destin_long_bucket))\n",
    "\n",
    "source_lati_bucket = (all_data['sourceLatitude']  // d_lati).as_matrix().astype(int)\n",
    "source_long_bucket = (all_data['sourceLongitude'] // d_long).as_matrix().astype(int)\n",
    "\n",
    "all_data['source_tuple'] = tuple(zip(source_lati_bucket,source_long_bucket))\n",
    "\n",
    "\n",
    "categorical_vars = ['date', 'SourceState', 'destinationState', 'vehicleType', \n",
    "                    'vehicleOption', 'source_tuple', 'destination_tuple']\n",
    "\n",
    "all_data = all_data.copy()\n",
    "categorical_var_encoders = {}\n",
    "for var in categorical_vars:\n",
    "    le = preprocessing.LabelEncoder().fit(all_data[var])\n",
    "    all_data[var + '_ids']  = le.transform(all_data[var])\n",
    "    all_data[var + '_ids']  = all_data[var + '_ids'].astype('int32')\n",
    "    all_data.pop(var)\n",
    "    categorical_var_encoders[var] = le\n",
    "\n",
    "#all_data = all_data.drop(['destinationLatitude', 'destinationLongitude', \n",
    "#                          'sourceLatitude', 'sourceLongitude', \n",
    "#                          'SourceState_ids', 'destinationState_ids' ],axis=1)\n",
    "    \n",
    "# The following two new features are required to achive the best current model \n",
    "all_data['source']           = all_data['sourceLatitude']*all_data['sourceLongitude']\n",
    "all_data['destination']      = all_data['destinationLatitude']*all_data['destinationLongitude']\n",
    "\n",
    "#continues_vars   = ['sourceLatitude', 'sourceLongitude', 'destinationLatitude', 'destinationLongitude',\n",
    "#                    'distanceKM', 'taxiDurationMin', 'weight',  'source', 'destination']\n",
    "\n",
    "#for cont in continues_vars:\n",
    "#    all_data[cont] = all_data[cont].astype('float32')\n",
    "#    #all_data[cont] = scale_minmax(all_data[cont].values)\n",
    "#    \n",
    "#all_data['distanceKM2']      = all_data['distanceKM']*all_data['distanceKM']\n",
    "#all_data['taxiDurationMin2'] = all_data['taxiDurationMin']*all_data['taxiDurationMin']\n",
    "#all_data['TaxiOverdistance'] = all_data['taxiDurationMin']/all_data['distanceKM']\n",
    "#all_data['TaxiOverdistance'] = all_data['TaxiOverdistance'].fillna(0)\n",
    "#all_data = all_data.drop(['taxiDurationMin'], axis=1)\n",
    "#all_data = all_data.drop(['sourceLatitude', 'sourceLongitude', 'destinationLatitude', 'destinationLongitude'], axis=1)\n",
    " \n",
    "#categorical_var_encoders['SourceState'].classes_\n",
    "    \n",
    "train    = all_data[:ntrain]\n",
    "test     = all_data[ntrain:]\n",
    "\n",
    "X = train.drop(['ID','price'],axis=1)\n",
    "y = train.price\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_bag = pd.read_pickle('train_bag-gboost-xgb-bag.pkl')\n",
    "\n",
    "#X_train, X_val = train_test_split(train_bag.drop(['ID','y_gboost'], axis=1), \n",
    "#                                          test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "#y_train = X_train.price\n",
    "#X_train = X_train.drop(['price'], axis=1)\n",
    "#y_val   = X_val.price\n",
    "#X_val   = X_val.drop(['price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3200, learning_rate=0.05,\n",
    "                                   max_depth=10, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "GBoost.fit(X_train, y_train)\n",
    "print( '%.2f' % float((time.time() - start_time)/60 ) )\n",
    "get_score(GBoost,X_val,y_val)\n",
    "# score = 17.344001531383512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=10, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =5 , nthread = -1)\n",
    "\n",
    "model_xgb.fit(X_train, y_train)\n",
    "print( '%.2f' % float((time.time() - start_time)/60 ) )\n",
    "get_score(model_xgb,X_val,y_val)\n",
    "\n",
    "# score = 18.431812261871737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag = BaggingRegressor(n_estimators=100, max_samples=1.0, max_features=1.0, random_state=5, verbose=1)\n",
    "Bag.fit(X_train, y_train)\n",
    "get_score(Bag,X_val,y_val)\n",
    "# 19.124410611169388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.91271925827475"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "\n",
    "clf_lasso = GridSearchCV(lasso, {'alpha': [1e-3,1e-2,1e-1,1,1e1,1e2,1e3]}, verbose=1)\n",
    "clf_lasso.fit(X_train,y_train)\n",
    "print(clf_lasso.best_params_)\n",
    "get_score(clf_lasso,X_val,y_val)\n",
    "# {'alpha': 100.0}\n",
    "# 40.78453998923231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lassolars  =  linear_model.LassoLars(alpha = 2)\n",
    "reg_lassolars.fit(X_train, y_train)\n",
    "get_score(reg_lassolars,X_val,y_val)\n",
    "# 40.775030093207896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.03838348612958"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_val = KNeighborsClassifier(2)\n",
    "knn_val.fit(X_train, y_train) \n",
    "get_score(knn_val,X_val,y_val)\n",
    "# 32.25103227931944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train,y_train)\n",
    "get_score(linear_reg,X_val,y_val)\n",
    "# 40.78006760576269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dec_tree = DecisionTreeRegressor(max_depth=15)\n",
    "reg_dec_tree.fit(X_train, y_train)\n",
    "get_score(reg_dec_tree,X_val,y_val)\n",
    "# 23.467060408266345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.8, l1_ratio=.9, random_state=3))\n",
    "ENet.fit(X_train, y_train)\n",
    "get_score(ENet,X_val,y_val)\n",
    "\n",
    "# score = 38.67009125705459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dec_tree = DecisionTreeRegressor(max_depth=10, criterion = 'mae', max_features='sqrt',\n",
    "                                     random_state=5, min_samples_leaf = 12, min_samples_split = 2)\n",
    "reg_dec_tree.fit(X_train, y_train)\n",
    "get_score(reg_dec_tree,X_val,y_val)\n",
    "# 26.002284134401865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "\n",
    "clf_ridge = GridSearchCV(ridge, {'alpha': [1e-3,1e-2,1e-1,1,1e1,1e2,1e3]}, verbose=1)\n",
    "\n",
    "clf_ridge.fit(X_train,y_train)\n",
    "print(clf_ridge.best_params_)\n",
    "get_score(clf_ridge,X_val,y_val)\n",
    "# 40.78025846231952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stale models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr_rbf = SVR(C= 1e-3, kernel = 'rbf' , gamma = 1e-3)\n",
    "#svr_rbf.fit(X_train,y_train)\n",
    "#get_score(svr_rbf,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr_rbf = SVR()\n",
    "\n",
    "#clf_svr = GridSearchCV(svr_rbf,\n",
    "#                   {'C': [1e-3],\n",
    "#                    'kernel': ['rbf'],\n",
    "#                    'gamma': [1e-3]}, verbose=1)\n",
    "\n",
    "#clf_svr.fit(X_train,y_train)\n",
    "#print(clf_svr.best_params_)\n",
    "#get_score(clf_svr,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_pipeline(RobustScaler(), Lasso(alpha =1, random_state=1))\n",
    "#lasso.fit(X_train, y_train)\n",
    "#get_score(lasso,X_val,y_val)\n",
    "\n",
    "# score = 53.923366658249186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computationally intensive \n",
    "#KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "#KRR.fit(X_train, y_train)\n",
    "#get_score(KRR,X_val,y_val)\n",
    "\n",
    "# score = 33.473933712491295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABoost = AdaBoostRegressor(n_estimators=1000, learning_rate=0.05, loss='linear', random_state=5)\n",
    "#ABoost.fit(X_train, y_train)\n",
    "#get_score(ABoost,X_val,y_val)\n",
    "\n",
    "# score = 58.54226269060756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_model = xgb.XGBRegressor()\n",
    "#clf = GridSearchCV(xgb_model,\n",
    "#                   {'max_depth': [2,4,6],\n",
    "#                    'n_estimators': [50,100,200]}, verbose=1)\n",
    "\n",
    "#clf.fit(X_train,y_train)\n",
    "#print(clf.best_params_)\n",
    "#get_score(clf,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RForest = RandomForestRegressor(n_estimators=20, criterion='mae', random_state = 5)\n",
    "##                                max_depth=4, max_features='sqrt',\n",
    "##                                min_samples_leaf=5, min_samples_split=3,\n",
    "##                                random_state = 42)\n",
    "#RForest.fit(X_train, y_train)\n",
    "#get_score(RForest,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dec_tree = DecisionTreeRegressor()\n",
    "\n",
    "#clf_dec_tree = GridSearchCV(dec_tree, \n",
    "#                            {'criterion' : ['mae'], \n",
    "#                             'max_depth' : [2,4,6,8,10,12,14,16,18,20,22,24],\n",
    "#                             'min_samples_split' : [2,4,6,8,10], \n",
    "#                             'min_samples_leaf' : [2,4,6,8,10,12,14,16,18,20,22,24],\n",
    "#                             'max_features' : ['sqrt'], \n",
    "#                             'random_state' : [5]\n",
    "#                            }, verbose=1)\n",
    "#clf_dec_tree.fit(X_train,y_train)\n",
    "#print(clf_dec_tree.best_params_)\n",
    "#get_score(clf_dec_tree,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_model = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "#                             learning_rate=0.05, min_child_weight=1.7817,\n",
    "#                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "#                             subsample=0.5213, silent=1, nthread = -1)\n",
    "\n",
    "#GBoost = GradientBoostingRegressor( learning_rate=0.05, max_features='sqrt',\n",
    "#                                   min_samples_leaf=15, min_samples_split=10, \n",
    "#                                   loss='huber')\n",
    "\n",
    "#Bag = BaggingRegressor()#base_estimator=KNeighborsRegressor())\n",
    "\n",
    "#regressors = [xgb_model, Bag]\n",
    "\n",
    "#stregr = StackingRegressor(regressors=regressors, meta_regressor=GBoost)\n",
    "\n",
    "#params = {'xgbregressor__n_estimators': [2000],#[100, 200, 500, 1000],\n",
    "#          'xgbregressor__max_depth': [8],#[2,4,8,16],\n",
    "#          'meta-gradientboostingregressor__n_estimators': [2000],#[100, 200, 500, 1000],\n",
    "#          'meta-gradientboostingregressor__max_depth': [48],#[2,4,8,16],\n",
    "#          'baggingregressor__base_estimator': [KNeighborsRegressor()]}#, DecisionTreeRegressor()]}\n",
    "\n",
    "#grid = GridSearchCV(estimator=stregr, \n",
    "#                    param_grid=params, \n",
    "#                    cv=3,\n",
    "#                    refit=True)\n",
    "#grid.fit(X_train, y_train)\n",
    "#print(grid.best_params_)\n",
    "#get_score(grid,X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_regressor = StackingRegressor(regressors=[model_xgb, Bag], meta_regressor = linear_reg)\n",
    "\n",
    "# Training the stacking classifier\n",
    "\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "get_score(stacking_regressor,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model_xgb.predict(X_val)\n",
    "pred2 = GBoost.predict(X_val)\n",
    "pred3 = Bag.predict(X_val)\n",
    "score1 = mean_absolute_precision_error(y_val, pred1)\n",
    "score2 = mean_absolute_precision_error(y_val, pred2)\n",
    "score3 = mean_absolute_precision_error(y_val, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pred(p_true,p1,p2,p3):\n",
    "    \n",
    "    min_score = 100\n",
    "    w1 = 0 \n",
    "    w2 = 0 \n",
    "    \n",
    "    for j in range(1,100):\n",
    "        for i in range(j,100):\n",
    "            p_pred = 0.01*i*p1+(1-0.01*i-0.01*j)*p1 + 0.01*j*p2 \n",
    "            score  = mean_absolute_precision_error(y_val, p_pred)\n",
    "            if score < min_score:\n",
    "                min_score = score \n",
    "                w1 = 0.01*i \n",
    "                w2 = 0.01*j \n",
    "    return min_score, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms,a1,a2 = max_pred(y_val,pred1,pred2,pred3)\n",
    "print(ms,a1,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model_xgb.predict(X_val)\n",
    "pred3 = Bag.predict(X_val)\n",
    "score1 = mean_absolute_precision_error(y_val, pred1)\n",
    "score3 = mean_absolute_precision_error(y_val, pred3)\n",
    "score  = mean_absolute_precision_error(y_val, pred)\n",
    "print(score1, score3, np.mean([score1,score3]), score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other's Stacking models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged base models class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models = (model_xgb, GBoost, Bag))\n",
    "averaged_models.fit(X_train, y_train)\n",
    "get_score(averaged_models,X_val,y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Averaged models Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                #instance.fit(X[train_index], y[train_index])\n",
    "                instance.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "                y_pred = instance.predict(X.iloc[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models  = StackingAveragedModels(base_models = (model_xgb, Bag),meta_model = linear_reg)\n",
    "stacked_averaged_models.fit(X_train, y_train)\n",
    "get_score(stacked_averaged_models,X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a new model for each Vehicle Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vTypes = set(all_data['vehicleType_ids'])\n",
    "all_vType     = {}\n",
    "final_models  = {}\n",
    "\n",
    "for i in vTypes:\n",
    "    tmp_vType = all_data[all_data['vehicleType_ids'] == i]\n",
    "    #tmp_vType = tmp_vType.drop(['vehicleType_ids'], axis=1)\n",
    "    all_vType[i] = tmp_vType\n",
    "    start_time = time.time()\n",
    "    vType_train = tmp_vType[tmp_vType['price'].notnull()]\n",
    "    vType_test  = tmp_vType[tmp_vType['price'].isnull()].drop(['price'], axis=1)\n",
    "    X = vType_train.drop(['ID','price', 'vehicleType_ids'],axis=1)\n",
    "    y = vType_train['price']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    if i != 1:\n",
    "        minSampleLeaf = 20 \n",
    "    else:\n",
    "        minSampleLeaf = 15\n",
    "    \n",
    "    if i != 3:\n",
    "        nEstimators = 1000\n",
    "        minSampleSplit = 4\n",
    "    else:\n",
    "        nEstimators = 1500\n",
    "        minSampleSplit = 2\n",
    "    \n",
    "    GBoost = GradientBoostingRegressor(learning_rate=0.1, max_features='sqrt', loss='huber',\n",
    "                                       min_samples_leaf=minSampleLeaf, min_samples_split=minSampleSplit, \n",
    "                                       n_estimators=nEstimators, max_depth=8, alpha = 0.7)\n",
    "    \n",
    "    GBoost.fit(X_train,y_train)\n",
    "    \n",
    "    print(\"%.2f\" % float((time.time() - start_time)/60 ) +\" mins, vType: \",i, \", items: \", tmp_vType.shape[0],\n",
    "          \", loss: \", \"%.2f\" % get_score(GBoost,X_val,y_val))\n",
    "    \n",
    "    GB_Final = GradientBoostingRegressor(learning_rate=0.1, max_features='sqrt', loss='huber',\n",
    "                                         min_samples_leaf=minSampleLeaf, min_samples_split=minSampleSplit, \n",
    "                                         n_estimators=nEstimators, max_depth=8, alpha = 0.7)\n",
    "    \n",
    "    GB_Final.fit(X,y)\n",
    "    final_models[i] = GB_Final  \n",
    "    \n",
    "    #clf_GBoost = GridSearchCV(GBoost, \n",
    "    #                      {\n",
    "    #                       'min_samples_split': [2,4],\n",
    "    #                       'n_estimators': [1000,1500], \n",
    "    #                       'max_depth': [4,8],\n",
    "    #                       'min_samples_leaf': [20,15],\n",
    "    #                      },  verbose=2)\n",
    "\n",
    "    #clf_GBoost.fit(X_train,y_train)\n",
    "    #, best model: \", GBoost.get_params)# clf_GBoost.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/mohsenkiskani/Downloads/Ubaar/submissions/submission15.csv\"\n",
    "with open(filename,\"w+\") as outputfile:\n",
    "    outputfile.write(\"ID,price\\n\")        \n",
    "    \n",
    "    for i in final_models.keys():\n",
    "        tmp_test0            = test[test['vehicleType_ids']==i]\n",
    "        tmp_test1            = tmp_test0.drop(['ID', 'price', 'vehicleType_ids'], axis=1)\n",
    "        tmp_model            = final_models[i]\n",
    "        tmp_test0['y_pred']  = tmp_model.predict(tmp_test1)\n",
    "    \n",
    "        for j in range(tmp_test0.shape[0]):\n",
    "            y_pred_test = tmp_test0.iloc[j]['y_pred'] \n",
    "            if y_pred_test < 0:\n",
    "                y_pred_test = min_price \n",
    "            outputfile.write(str(int(tmp_test0.iloc[j]['ID'] ))+\",\"+str(int(np.ceil(y_pred_test)))+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a new model for each Vehicle Type and Vehicle Option ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cats2 = all_data[['vehicleType_ids', 'vehicleOption_ids']].as_matrix().tolist()\n",
    "all_cats2 = [(x[0],x[1]) for x in all_cats2 ]\n",
    "all_cats2 = set(all_cats2)\n",
    "all_data_cat2 = {}\n",
    "\n",
    "for item in all_cats2:\n",
    "    tmp_data1 = all_data[all_data['vehicleType_ids'] == item[0]]\n",
    "    tmp_data2 = tmp_data1[tmp_data1['vehicleOption_ids'] == item[1]]\n",
    "    tmp_data = tmp_data2.drop(['vehicleType_ids', 'vehicleOption_ids'], axis=1)\n",
    "    all_data_cat2[item] = tmp_data\n",
    "    #print(item, tmp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cat_shape2 = {}\n",
    "for x in all_data_cat2.keys():\n",
    "    \n",
    "    cat_df2    = all_data_cat2[x]\n",
    "    cat_train2 = cat_df2[cat_df2['price'].notnull()]\n",
    "    cat_test2  = cat_df2[cat_df2['price'].isnull()].drop(['price'], axis=1)\n",
    "    \n",
    "    all_data_cat_shape2[x] = (cat_train2.shape[0], cat_test2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sum = pd.DataFrame() \n",
    "for item in  [(1,6)]: # all_cats2: \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cat_df2    = all_data_cat2[item]\n",
    "    cat_train2 = cat_df2[cat_df2['price'].notnull()]\n",
    "    cat_test2  = cat_df2[cat_df2['price'].isnull()].drop(['price'], axis=1)\n",
    "\n",
    "    X = cat_train2.drop(['ID','price'],axis=1)\n",
    "    y = cat_train2['price']\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    #gboost_base = GradientBoostingRegressor()\n",
    "    #GBoost      = GridSearchCV(gboost_base,\n",
    "    #                   {'max_depth': [2,4,6,8,10,12],\n",
    "    #                    'n_estimators': [50,100,200,500,1000]}, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    GBoost = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05,\n",
    "                                       max_depth=6, max_features='sqrt',\n",
    "                                       min_samples_leaf=8, min_samples_split=4, \n",
    "                                       loss='huber', random_state =5)\n",
    "\n",
    "    GBoost.fit(X_train, y_train)\n",
    "    #print(GBoost.best_params_)\n",
    "    #'destinationLatitude','destinationLongitude','sourceLatitude','sourceLongitude',\n",
    "    print(item, cat_df2.shape[0], \"%.2f\" % get_score(GBoost,X_val,y_val),\n",
    "          \"%.2f\" % float((time.time() - start_time)/60 )) \n",
    "\n",
    "\n",
    "#cat_test['pred_price'] = GBoost.predict(cat_test.drop(['ID'], axis=1))\n",
    "#cat_test['pred_price'] = cat_test['pred_price'].apply((lambda x: max(x, min_price) ))\n",
    "#test_sum = pd.concat([test_sum,cat_test])\n",
    "\n",
    "#test_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current best model \n",
    "start_time = time.time()\n",
    "Final = GradientBoostingRegressor(n_estimators=2200, learning_rate=0.05,\n",
    "                                   max_depth=10, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "Final.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "y_pred_test = Final.predict(test.drop(['ID','price'],axis=1))\n",
    "print( '%.2f' % float((time.time() - start_time)/60 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/mohsenkiskani/Downloads/Ubaar/submissions/submission17.csv\"\n",
    "with open(filename,\"w+\") as outputfile:\n",
    "    outputfile.write(\"ID,price\\n\")\n",
    "    for i in range(y_pred_test.shape[0]):\n",
    "        if y_pred_test[i] < 0:\n",
    "            y_pred_test[i] = 100000 \n",
    "        outputfile.write(str(test_data.ID[i])+\",\"+str(int(np.ceil(y_pred_test[i])))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with high skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "\n",
    "#print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "#print(skewness.head(10))\n",
    "\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "#print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "#all_data[skewed_features] = np.log1p(all_data[skewed_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, (ay1, ay2) = plt.subplots(2, 1)\n",
    "\n",
    "\n",
    "ay1.scatter(x = X_train['sourceLatitude'], y = y_train, marker = \"+\")\n",
    "ay1.set_title('Price vs sourceLatitude')\n",
    "ay1.set_xlabel('sourceLatitude')\n",
    "ay1.set_ylabel('Price')\n",
    "\n",
    "ay2.scatter(x = X_train['sourceLongitude'], y = y_train, marker = \"+\")\n",
    "ay2.set_title('Price vs sourceLongitude')\n",
    "ay2.set_xlabel('sourceLongitude')\n",
    "ay2.set_ylabel('Price')\n",
    "\n",
    "fig1.set_size_inches(28.5, 10.5)\n",
    "fig1.savefig(\"/Users/mohsenkiskani/Downloads/Ubaar/plots/sourceEffects.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, (ay3, ay4) = plt.subplots(2, 1)\n",
    "\n",
    "\n",
    "ay3.scatter(x = X_train['destinationLatitude'], y = y_train, marker = \"+\")\n",
    "ay3.set_title('Price vs destinationLatitude')\n",
    "ay3.set_xlabel('destinationLatitude')\n",
    "ay3.set_ylabel('Price')\n",
    "\n",
    "ay4.scatter(x = X_train['destinationLongitude'], y = y_train, marker = \"+\")\n",
    "ay4.set_title('Price vs destinationLongitude')\n",
    "ay4.set_xlabel('destinationLongitude')\n",
    "ay4.set_ylabel('Price')\n",
    "\n",
    "fig2.set_size_inches(28.5, 10.5)\n",
    "fig2.savefig(\"/Users/mohsenkiskani/Downloads/Ubaar/plots/destinationEffects.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, (ay3, ay4, ay5) = plt.subplots(3, 1)\n",
    "\n",
    "\n",
    "ay3.scatter(x = X_train['distanceKM'], y = y_train, marker = \"+\")\n",
    "ay3.set_title('Price vs distanceKM')\n",
    "ay3.set_xlabel('distanceKM')\n",
    "ay3.set_ylabel('Price')\n",
    "\n",
    "ay4.scatter(x = X_train['taxiDurationMin'], y = y_train, marker = \"+\")\n",
    "ay4.set_title('Price vs taxiDurationMin')\n",
    "ay4.set_xlabel('taxiDurationMin')\n",
    "ay4.set_ylabel('Price')\n",
    "\n",
    "ay5.scatter(x = X_train['weight'], y = y_train, marker = \"+\")\n",
    "ay5.set_title('Price vs weight')\n",
    "ay5.set_xlabel('weight')\n",
    "ay5.set_ylabel('Price')\n",
    "\n",
    "fig3.set_size_inches(28.5, 10.5)\n",
    "fig3.savefig(\"/Users/mohsenkiskani/Downloads/Ubaar/plots/distance-time-Effects.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = data.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_train , fit=norm);\n",
    "(mu, sigma) = norm.fit(y_train)\n",
    "\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(y_train, plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lp = np.log1p(y_train)\n",
    "\n",
    "sns.distplot(y_train_lp, fit=norm);\n",
    "(mu, sigma) = norm.fit(y_train_lp)\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(y_train_lp, plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = data['date'].unique()\n",
    "date_price_vec = []\n",
    "\n",
    "for date in all_dates:\n",
    "    date_price_vec.append(np.mean(data.query('date == ' + str(date) ).price))\n",
    "    \n",
    "states = all_data['SourceState_ids'].unique()\n",
    "source_price_vec = []\n",
    "destin_price_vec = []\n",
    "\n",
    "for state in states:\n",
    "    source_price_vec.append(np.mean(data.query('date == ' + str(state) ).price))\n",
    "    destin_price_vec.append(np.mean(data.query('date == ' + str(state) ).price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, (ay1, ay2, ay3, ay4, ay5) = plt.subplots(5, 1)\n",
    "\n",
    "\n",
    "ay1.scatter(data.vehicleType, data.price, marker = \"+\")\n",
    "ay1.plot([0, 1, 2, 3], [np.mean(data.query('vehicleType == 0').price), np.mean(data.query('vehicleType == 1').price),\n",
    "         np.mean(data.query('vehicleType == 2').price), np.mean(data.query('vehicleType == 3').price)], 'r--')\n",
    "\n",
    "ay1.set_title('Price vs vehicle type')\n",
    "ay1.set_xlabel('Vehicle Type')\n",
    "ay1.set_ylabel('Price')\n",
    "\n",
    "\n",
    "ay2.scatter(data.vehicleOption, data.price, marker = \"+\")\n",
    "ay2.plot([0, 1, 2, 3, 4, 5, 6, 7, 8], [np.mean(data.query('vehicleOption == 0').price), \n",
    "                                       np.mean(data.query('vehicleOption == 1').price),\n",
    "                                       np.mean(data.query('vehicleOption == 2').price), \n",
    "                                       np.mean(data.query('vehicleOption == 3').price), \n",
    "                                       np.mean(data.query('vehicleOption == 4').price), \n",
    "                                       np.mean(data.query('vehicleOption == 5').price),\n",
    "                                       np.mean(data.query('vehicleOption == 6').price),\n",
    "                                       np.mean(data.query('vehicleOption == 7').price), \n",
    "                                       np.mean(data.query('vehicleOption == 8').price)], 'r--')\n",
    "\n",
    "\n",
    "ay2.set_title('Price vs vehicle option')\n",
    "ay2.set_xlabel('Vehicle Option')\n",
    "ay2.set_ylabel('Price')\n",
    "\n",
    "ay3.scatter(data.date, data.price, marker = \"+\")\n",
    "ay3.plot(all_dates, date_price_vec, 'r--')\n",
    "ay3.set_title('Price vs date')\n",
    "ay3.set_xlabel('Date')\n",
    "ay3.set_ylabel('Price')\n",
    "\n",
    "ay4.scatter(data.SourceState, data.price, marker = \"+\")\n",
    "ay4.plot(states, source_price_vec, 'r--')\n",
    "ay4.set_title('Price vs source state')\n",
    "ay4.set_xlabel('Date')\n",
    "ay4.set_ylabel('Price')\n",
    "\n",
    "ay5.scatter(data.destinationState, data.price, marker = \"+\")\n",
    "ay5.plot(states, destin_price_vec, 'r--')\n",
    "ay5.set_title('Price vs destination state')\n",
    "ay5.set_xlabel('Date')\n",
    "ay5.set_ylabel('Price')\n",
    "\n",
    "fig1.set_size_inches(28.5, 10.5)\n",
    "fig1.savefig(\"/Users/mohsenkiskani/Downloads/Ubaar/plots/categoryEffects.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which features are most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.scatter(train[col], train['price'])\n",
    "    plt.title('Price vs '+ col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(train['destinationLatitude'], train['destinationLongitude'], train['price'], c='r', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(train['sourceLatitude'], train['sourceLongitude'], train['price'], c='g', marker='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#all_data['destination_tuple'] = all_data['destination_tuple'].apply(lambda x: (int(x[0], int(x[1]))))\n",
    "#for i in range(all_data.shape[0]):\n",
    "#    x = all_data['destinationLatitude'].iloc[i]\n",
    "#    y = all_data['destinationLongitude'].iloc[i]\n",
    "    \n",
    "#    lati_bucket = x // d_lati\n",
    "#    long_bucket = y // d_long\n",
    "    \n",
    "#    all_data['destination_tuple'].iloc[i] = (lati_bucket, long_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
