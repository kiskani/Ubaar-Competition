{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "#from scipy.special import boxcox1p\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor \n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "from mlxtend.regressor import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mean_absolute_precision_error(y_pred, y_true):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data      = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/train.csv')\n",
    "test_data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/ubaar-competition/test.csv')\n",
    "\n",
    "# Remove NANs\n",
    "data      = data.dropna(axis = 0)\n",
    "\n",
    "# Remove outliers\n",
    "data.drop([28098])\n",
    "THRESHOLD = 3.5e7\n",
    "Aa = data[data.price > THRESHOLD]\n",
    "data = data.drop(Aa.index.tolist())\n",
    "\n",
    "specific_cols = ['distanceKM', 'taxiDurationMin', 'weight']\n",
    "removed_indices = []\n",
    "for col in specific_cols:\n",
    "    df = data['price']/data[col]\n",
    "    A = df[~df.isin([np.nan, np.inf, -np.inf])]\n",
    "    B = (A - np.mean(A)) / np.std(A)\n",
    "    V = B[B > 5]\n",
    "    removed_indices.extend(V.index.tolist())\n",
    "data = data.drop(set(removed_indices))\n",
    "\n",
    "# Fill test NANs\n",
    "test_data.loc[12577, 'distanceKM']      = 52\n",
    "test_data.loc[12577, 'taxiDurationMin'] = 50\n",
    "test_data.loc[13853, 'distanceKM']      = 500\n",
    "test_data.loc[13853, 'taxiDurationMin'] = 380\n",
    "\n",
    "all_data = pd.concat((data, test_data)) \n",
    "all_data['source']           = all_data['sourceLatitude']*all_data['sourceLongitude']\n",
    "all_data['destination']      = all_data['destinationLatitude']*all_data['destinationLongitude']\n",
    "\n",
    "ntrain = data.shape[0]\n",
    "ntest  = test_data.shape[0]\n",
    "\n",
    "categorical_vars = ['date', 'SourceState', 'destinationState', 'vehicleType', 'vehicleOption']\n",
    "\n",
    "dummies_data = pd.get_dummies(all_data[categorical_vars])\n",
    "all_data[dummies_data.columns] = dummies_data[dummies_data.columns]\n",
    "all_data.drop(categorical_vars, axis=1, inplace=True)\n",
    "\n",
    "train    = all_data[:ntrain]\n",
    "test     = all_data[ntrain:]\n",
    "\n",
    "#X = train.drop(['ID','price'],axis=1)\n",
    "#y = train.price\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost_1 = GradientBoostingRegressor(n_estimators=15000, learning_rate=0.01,\n",
    "                                     max_depth=10, max_features='sqrt',\n",
    "                                     min_samples_leaf=15, min_samples_split=10, loss='huber')\n",
    "\n",
    "GBoost_2 = GradientBoostingRegressor(n_estimators=15000, learning_rate=0.01,\n",
    "                                  max_depth=10, max_features='sqrt',\n",
    "                                  min_samples_leaf=15, min_samples_split=10, loss='huber')\n",
    "\n",
    "xgb_1 = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                        learning_rate=0.01, max_depth=12, \n",
    "                        min_child_weight=1.7817, n_estimators=8000,\n",
    "                        reg_alpha=0.9640, reg_lambda=0.8571,\n",
    "                        subsample=1, silent=1, nthread = -1)\n",
    "\n",
    "xgb_2 = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                        learning_rate=0.01, max_depth=12, \n",
    "                        min_child_weight=1.7817, n_estimators=8000,\n",
    "                        reg_alpha=0.9640, reg_lambda=0.8571,\n",
    "                        subsample=1, silent=1, nthread = -1)\n",
    "\n",
    "lgb_1 = lgb.LGBMRegressor(objective='regression',num_leaves=25, #save_binary = True,  \n",
    "                          learning_rate=0.01, n_estimators=60000,\n",
    "                          max_bin = 150, bagging_fraction = 0.95,\n",
    "                          bagging_freq = 4, feature_fraction = 0.8,\n",
    "                          feature_fraction_seed=50, bagging_seed=20,\n",
    "                          min_data_in_leaf = 11, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "lgb_2 = lgb.LGBMRegressor(objective='regression',num_leaves=25, #save_binary = True,  \n",
    "                          learning_rate=0.01, n_estimators=60000,\n",
    "                          max_bin = 150, bagging_fraction = 0.95,\n",
    "                          bagging_freq = 4, feature_fraction = 0.8,\n",
    "                          feature_fraction_seed=50, bagging_seed=20,\n",
    "                          min_data_in_leaf = 11, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_11, train_12 = train_test_split(train, test_size=0.5)\n",
    "\n",
    "GBoost_1.fit(train_11.drop(['ID','price'],axis=1), train_11.price)\n",
    "GBoost_2.fit(train_12.drop(['ID','price'],axis=1), train_12.price)\n",
    "\n",
    "train_11['y_gboost'] = GBoost_2.predict(train_11.drop(['ID','price'],axis=1))\n",
    "train_12['y_gboost'] = GBoost_1.predict(train_12.drop(['ID','price'],axis=1))\n",
    "\n",
    "train_1 = pd.concat([train_11, train_12]) \n",
    "\n",
    "train_21, train_22 = train_test_split(train_1, test_size=0.5)\n",
    "\n",
    "xgb_1.fit(train_21.drop(['ID','price', 'y_gboost'],axis=1), train_21.price)\n",
    "xgb_2.fit(train_22.drop(['ID','price', 'y_gboost'],axis=1), train_22.price)\n",
    "\n",
    "train_21['y_xgb'] = xgb_2.predict(train_21.drop(['ID','price', 'y_gboost'],axis=1))\n",
    "train_22['y_xgb'] = xgb_1.predict(train_22.drop(['ID','price', 'y_gboost'],axis=1))\n",
    "\n",
    "train_2 = pd.concat([train_21, train_22])\n",
    "\n",
    "train_31, train_32 = train_test_split(train_2, test_size=0.5)\n",
    "\n",
    "lgb_1.fit(train_31.drop(['ID','price', 'y_gboost', 'y_xgb'],axis=1), train_31.price)\n",
    "lgb_2.fit(train_32.drop(['ID','price', 'y_gboost', 'y_xgb'],axis=1), train_32.price)\n",
    "\n",
    "train_31['y_lgb'] = lgb_2.predict(train_31.drop(['ID','price', 'y_gboost', 'y_xgb'],axis=1))\n",
    "train_32['y_lgb'] = lgb_1.predict(train_32.drop(['ID','price', 'y_gboost', 'y_xgb'],axis=1))\n",
    "\n",
    "train_3 = pd.concat([train_31, train_32])\n",
    "train_3.to_pickle('dataFrames/train_OneHotEncoding_new_June14th.pkl')\n",
    "\n",
    "print( '%.2f' % float((time.time() - start_time)/60 )  + ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.20 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_41, train_42 = train_test_split(train_3, test_size=0.5)\n",
    "\n",
    "avg_model_1 = AveragingModels(models = (xgb_1, lgb_1))\n",
    "avg_model_2 = AveragingModels(models = (xgb_2, lgb_2))\n",
    "\n",
    "avg_model_1.fit(train_41.drop(['ID','price', 'y_gboost', 'y_xgb', 'y_lgb'],axis=1), train_41.price)\n",
    "avg_model_2.fit(train_42.drop(['ID','price', 'y_gboost', 'y_xgb', 'y_lgb'],axis=1), train_42.price)\n",
    "\n",
    "train_41['y_avg_lgb_xgb'] = avg_model_2.predict(train_41.drop(['ID','price', 'y_gboost', 'y_xgb', 'y_lgb'],axis=1))\n",
    "train_42['y_avg_lgb_xgb'] = avg_model_1.predict(train_42.drop(['ID','price', 'y_gboost', 'y_xgb', 'y_lgb'],axis=1))\n",
    "\n",
    "train_4 = pd.concat([train_41, train_42])\n",
    "train_4.to_pickle('dataFrames/train_OneHotEncoding_new_June14th.pkl')\n",
    "\n",
    "print( '%.2f' % float((time.time() - start_time)/60 )  + ' mins')\n",
    "# 45.20 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.00 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "GBoost = GradientBoostingRegressor(n_estimators=15000, learning_rate=0.01,\n",
    "                                     max_depth=10, max_features='sqrt',\n",
    "                                     min_samples_leaf=15, min_samples_split=10, loss='huber')\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.01, max_depth=12, \n",
    "                             min_child_weight=1.7817, n_estimators=8000,\n",
    "                             reg_alpha=0.9640, reg_lambda=0.8571,\n",
    "                             subsample=1, silent=1, nthread = -1)\n",
    "\n",
    "lgb_m = lgb.LGBMRegressor(objective='regression',num_leaves=25, #save_binary = True,  \n",
    "                          learning_rate=0.01, n_estimators=60000,\n",
    "                          max_bin = 150, bagging_fraction = 0.95,\n",
    "                          bagging_freq = 4, feature_fraction = 0.8,\n",
    "                          feature_fraction_seed=50, bagging_seed=20,\n",
    "                          min_data_in_leaf = 11, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "avg_model = AveragingModels(models = (xgb_model, lgb_m))\n",
    "\n",
    "GBoost.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "xgb_model.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "lgb_m.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "avg_model.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "\n",
    "test['y_gboost']      = GBoost.predict(test.drop(['ID','price'],axis=1))\n",
    "test['y_xgb']         = xgb_model.predict(test.drop(['ID','price','y_gboost'],axis=1))\n",
    "test['y_lgb']         = lgb_m.predict(test.drop(['ID','price','y_gboost','y_xgb'],axis=1))\n",
    "test['y_avg_lgb_xgb'] = avg_model.predict(test.drop(['ID','price','y_gboost','y_xgb', 'y_lgb'],axis=1))\n",
    "\n",
    "test.to_pickle('dataFrames/test_OneHotEncoding_new_June14th.pkl')\n",
    "print( '%.2f' % float((time.time() - start_time)/60 ), \"mins\" )\n",
    "\n",
    "# 108.00 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More training features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_1 = BaggingRegressor(n_estimators=1000, max_samples=1.0, max_features=1.0, verbose=1)\n",
    "bag_2 = BaggingRegressor(n_estimators=1000, max_samples=1.0, max_features=1.0, verbose=1)\n",
    "\n",
    "knn_1 = KNeighborsClassifier(2)\n",
    "knn_2 = KNeighborsClassifier(2)\n",
    "\n",
    "dec_1 = DecisionTreeRegressor(max_depth=10)\n",
    "dec_2 = DecisionTreeRegressor(max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.9min finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.9min finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.6s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.37 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_4 = pd.read_pickle('dataFrames/train_OneHotEncoding_new_June14th.pkl')\n",
    "train_51, train_52 = train_test_split(train_4, test_size=0.5)\n",
    "\n",
    "bag_1.fit(train_51.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb'],axis=1), train_51.price)\n",
    "bag_2.fit(train_52.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb'],axis=1), train_52.price)\n",
    "\n",
    "train_51['y_bag'] = bag_2.predict(train_51.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb'],axis=1))\n",
    "train_52['y_bag'] = bag_1.predict(train_52.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb'],axis=1))\n",
    "\n",
    "train_5 = pd.concat([train_51, train_52])\n",
    "train_61, train_62 = train_test_split(train_5, test_size=0.5)\n",
    "\n",
    "knn_1.fit(train_61.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb', 'y_bag'],axis=1), train_61.price)\n",
    "knn_2.fit(train_62.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb', 'y_bag'],axis=1), train_62.price)\n",
    "\n",
    "train_61['y_knn'] = knn_2.predict(train_61.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb', 'y_bag'],axis=1))\n",
    "train_62['y_knn'] = knn_1.predict(train_62.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', 'y_avg_lgb_xgb', 'y_bag'],axis=1))\n",
    "\n",
    "train_6 = pd.concat([train_61, train_62])\n",
    "train_71, train_72 = train_test_split(train_6, test_size=0.5)\n",
    "\n",
    "dec_1.fit(train_71.drop(['ID','price','y_gboost','y_xgb', 'y_lgb',\n",
    "                         'y_avg_lgb_xgb', 'y_bag', 'y_knn'],axis=1), train_71.price)\n",
    "dec_2.fit(train_72.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', \n",
    "                         'y_avg_lgb_xgb', 'y_bag', 'y_knn'],axis=1), train_72.price)\n",
    "\n",
    "train_71['y_dec'] = dec_2.predict(train_71.drop(['ID','price','y_gboost','y_xgb', 'y_lgb',\n",
    "                                                 'y_avg_lgb_xgb', 'y_bag', 'y_knn'],axis=1))\n",
    "train_72['y_dec'] = dec_1.predict(train_72.drop(['ID','price','y_gboost','y_xgb', 'y_lgb',\n",
    "                                                 'y_avg_lgb_xgb', 'y_bag', 'y_knn'],axis=1))\n",
    "\n",
    "train_7 = pd.concat([train_71, train_72])\n",
    "\n",
    "train_7.to_pickle('dataFrames/train_OneHotEncoding_new_June14th.pkl')\n",
    "print( '%.2f' % float((time.time() - start_time)/60 )  + ' mins' )\n",
    "# 10.37 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Test Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 10.8min finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.01 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>destinationLatitude</th>\n",
       "      <th>destinationLongitude</th>\n",
       "      <th>distanceKM</th>\n",
       "      <th>price</th>\n",
       "      <th>sourceLatitude</th>\n",
       "      <th>sourceLongitude</th>\n",
       "      <th>taxiDurationMin</th>\n",
       "      <th>weight</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>vehicleOption_mosaghaf_felezi</th>\n",
       "      <th>vehicleOption_transit_chadori</th>\n",
       "      <th>vehicleOption_yakhchali</th>\n",
       "      <th>y_gboost</th>\n",
       "      <th>y_xgb</th>\n",
       "      <th>y_lgb</th>\n",
       "      <th>y_avg_lgb_xgb</th>\n",
       "      <th>y_bag</th>\n",
       "      <th>y_knn</th>\n",
       "      <th>y_dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10010571124</td>\n",
       "      <td>35.579635</td>\n",
       "      <td>53.384990</td>\n",
       "      <td>684.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.297213</td>\n",
       "      <td>59.607970</td>\n",
       "      <td>446.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>2163.603184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.650851e+06</td>\n",
       "      <td>2474225.5</td>\n",
       "      <td>2.513541e+06</td>\n",
       "      <td>2.493883e+06</td>\n",
       "      <td>2564513.0</td>\n",
       "      <td>2200000.0</td>\n",
       "      <td>2.987105e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10031704713</td>\n",
       "      <td>29.605761</td>\n",
       "      <td>52.533588</td>\n",
       "      <td>931.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.704695</td>\n",
       "      <td>51.405194</td>\n",
       "      <td>614.0</td>\n",
       "      <td>19.14</td>\n",
       "      <td>1835.406773</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.678339e+06</td>\n",
       "      <td>9890198.0</td>\n",
       "      <td>1.023780e+07</td>\n",
       "      <td>1.006400e+07</td>\n",
       "      <td>9488653.0</td>\n",
       "      <td>8767000.0</td>\n",
       "      <td>1.022329e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10040911649</td>\n",
       "      <td>36.299593</td>\n",
       "      <td>59.612010</td>\n",
       "      <td>1469.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.948490</td>\n",
       "      <td>55.583875</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1497.901500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.071762e+07</td>\n",
       "      <td>17279658.0</td>\n",
       "      <td>2.155472e+07</td>\n",
       "      <td>1.941719e+07</td>\n",
       "      <td>17182852.0</td>\n",
       "      <td>8687000.0</td>\n",
       "      <td>1.450000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10047106840</td>\n",
       "      <td>35.248298</td>\n",
       "      <td>58.457567</td>\n",
       "      <td>745.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.339066</td>\n",
       "      <td>52.075970</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1840.316141</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.059601e+06</td>\n",
       "      <td>2124303.0</td>\n",
       "      <td>2.179348e+06</td>\n",
       "      <td>2.151826e+06</td>\n",
       "      <td>2360688.0</td>\n",
       "      <td>2170000.0</td>\n",
       "      <td>3.974901e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10050126039</td>\n",
       "      <td>34.636832</td>\n",
       "      <td>50.874888</td>\n",
       "      <td>281.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.579577</td>\n",
       "      <td>53.394403</td>\n",
       "      <td>181.0</td>\n",
       "      <td>23.50</td>\n",
       "      <td>1899.750273</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.039649e+06</td>\n",
       "      <td>6718747.5</td>\n",
       "      <td>6.023552e+06</td>\n",
       "      <td>6.371150e+06</td>\n",
       "      <td>7267524.0</td>\n",
       "      <td>1900000.0</td>\n",
       "      <td>5.984402e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  destinationLatitude  destinationLongitude  distanceKM  price  \\\n",
       "0  10010571124            35.579635             53.384990       684.0    NaN   \n",
       "1  10031704713            29.605761             52.533588       931.0    NaN   \n",
       "2  10040911649            36.299593             59.612010      1469.0    NaN   \n",
       "3  10047106840            35.248298             58.457567       745.0    NaN   \n",
       "4  10050126039            34.636832             50.874888       281.0    NaN   \n",
       "\n",
       "   sourceLatitude  sourceLongitude  taxiDurationMin  weight       source  \\\n",
       "0       36.297213        59.607970            446.0    2.33  2163.603184   \n",
       "1       35.704695        51.405194            614.0   19.14  1835.406773   \n",
       "2       26.948490        55.583875           1009.0   22.00  1497.901500   \n",
       "3       35.339066        52.075970            496.0    2.50  1840.316141   \n",
       "4       35.579577        53.394403            181.0   23.50  1899.750273   \n",
       "\n",
       "       ...       vehicleOption_mosaghaf_felezi  vehicleOption_transit_chadori  \\\n",
       "0      ...                                   0                              0   \n",
       "1      ...                                   0                              0   \n",
       "2      ...                                   0                              0   \n",
       "3      ...                                   1                              0   \n",
       "4      ...                                   0                              0   \n",
       "\n",
       "   vehicleOption_yakhchali      y_gboost       y_xgb         y_lgb  \\\n",
       "0                        0  2.650851e+06   2474225.5  2.513541e+06   \n",
       "1                        0  9.678339e+06   9890198.0  1.023780e+07   \n",
       "2                        0  2.071762e+07  17279658.0  2.155472e+07   \n",
       "3                        0  2.059601e+06   2124303.0  2.179348e+06   \n",
       "4                        0  6.039649e+06   6718747.5  6.023552e+06   \n",
       "\n",
       "   y_avg_lgb_xgb       y_bag      y_knn         y_dec  \n",
       "0   2.493883e+06   2564513.0  2200000.0  2.987105e+06  \n",
       "1   1.006400e+07   9488653.0  8767000.0  1.022329e+07  \n",
       "2   1.941719e+07  17182852.0  8687000.0  1.450000e+07  \n",
       "3   2.151826e+06   2360688.0  2170000.0  3.974901e+06  \n",
       "4   6.371150e+06   7267524.0  1900000.0  5.984402e+06  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test = pd.read_pickle('dataFrames/test_OneHotEncoding_new_June14th.pkl')\n",
    "\n",
    "bag = BaggingRegressor(n_estimators=1000, max_samples=1.0, max_features=1.0, verbose=1)\n",
    "knn = KNeighborsClassifier(2)\n",
    "dec = DecisionTreeRegressor(max_depth=10)\n",
    "\n",
    "bag.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "knn.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "dec.fit(train.drop(['ID','price'],axis=1), train.price)\n",
    "\n",
    "test['y_bag']    = bag.predict(test.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', \n",
    "                                          'y_avg_lgb_xgb'],axis=1))\n",
    "test['y_knn']    = knn.predict(test.drop(['ID','price','y_gboost','y_xgb', 'y_lgb',\n",
    "                                          'y_avg_lgb_xgb', 'y_bag'],axis=1))\n",
    "test['y_dec']    = dec.predict(test.drop(['ID','price','y_gboost','y_xgb', 'y_lgb', \n",
    "                                          'y_avg_lgb_xgb', 'y_bag', 'y_knn'],axis=1))\n",
    "\n",
    "test.to_pickle('dataFrames/test_OneHotEncoding_new_June14th.pkl')\n",
    "print( '%.2f' % float((time.time() - start_time)/60 ), \"mins\" )\n",
    "test.head()\n",
    "# 11.01 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 93)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
