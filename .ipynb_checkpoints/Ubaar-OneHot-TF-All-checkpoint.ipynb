{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mean_absolute_precision_error(y_pred, y_true):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test  = pd.read_pickle('dataFrames/test_OneHotEncoding.pkl')\n",
    "#train = pd.read_pickle('dataFrames/train_OneHotEncoding.pkl')\n",
    "test   = pd.read_pickle('dataFrames/test_OneHotEncoding_new_June14th.pkl')\n",
    "train  = pd.read_pickle('dataFrames/train_OneHotEncoding_new_June14th.pkl')\n",
    "\n",
    "\n",
    "continuous_cols = ['destinationLatitude', 'destinationLongitude', 'distanceKM', 'sourceLatitude', \n",
    "                   'sourceLongitude', 'taxiDurationMin', 'weight', 'source', 'destination', \n",
    "                   'y_gboost', 'y_xgb', 'y_bag', 'y_knn', 'y_dec', 'y_lgb' ]\n",
    "categorical_cols = train.columns.drop(continuous_cols + ['ID', 'price']).tolist()\n",
    "\n",
    "NOM = train[categorical_cols].shape[1]\n",
    "renaming_dict = dict(zip(train[categorical_cols].columns, [str(x) for x in list(range(NOM)) ]))\n",
    "\n",
    "train_renamed = train[categorical_cols].rename(columns=renaming_dict)\n",
    "test_renamed  = test[categorical_cols].rename(columns=renaming_dict)\n",
    "\n",
    "for column in continuous_cols:\n",
    "    train_renamed[column] = train[column]\n",
    "    test_renamed[column] = test[column]\n",
    "    \n",
    "test_renamed['ID']   = test['ID']\n",
    "train_renamed['ID'] = train['ID']\n",
    "test_renamed['price'] = test['price']\n",
    "train_renamed['price'] = train['price']\n",
    "\n",
    "X_train, X_val = train_test_split(train_renamed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow combination  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE          = 128\n",
    "TRAIN_EPOCHS        = 400\n",
    "HIDDEN_LAYER_1_SIZE = 64\n",
    "HIDDEN_LAYER_2_SIZE = 64\n",
    "HIDDEN_LAYER_3_SIZE = 16\n",
    "lr                  = 5e-5\n",
    "USE_ALL_FEATURES    = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(features, labels, mode, params, config):\n",
    "    input_layer = tf.feature_column.input_layer(features=features, feature_columns=feature_columns)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    x = tf.layers.dense(inputs=input_layer, units=HIDDEN_LAYER_1_SIZE, activation=tf.nn.relu, name=\"first_layer\")\n",
    "    x = tf.layers.dropout(inputs=x,name=\"first_dropout\")\n",
    "    x = tf.layers.dense(inputs=x, units=HIDDEN_LAYER_2_SIZE, activation=tf.nn.relu, name=\"second_layer\")\n",
    "    x = tf.layers.dense(inputs=x, units=HIDDEN_LAYER_3_SIZE, activation=tf.nn.relu, name=\"third_layer\")\n",
    "    predictions = tf.contrib.layers.fully_connected(inputs=x, num_outputs=1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT :\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss  = tf.reduce_mean(tf.abs(tf.divide(predictions-labels,labels))) \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, loss=loss)\n",
    "    else:\n",
    "        loss  = tf.reduce_mean(tf.abs(tf.divide(predictions-labels,labels))) \n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = set()\n",
    "\n",
    "if USE_ALL_FEATURES:\n",
    "    for col in categorical_cols:\n",
    "        col_feat = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(renaming_dict[col], 2),2)\n",
    "        feature_columns.add(col_feat)\n",
    "\n",
    "for cont in continuous_cols:\n",
    "    col_feat = tf.feature_column.numeric_column(cont)\n",
    "    feature_columns.add(col_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(df, pred = False, use_all_features = USE_ALL_FEATURES):\n",
    "        \n",
    "    useful_fueatures = list()\n",
    "    \n",
    "    if use_all_features:\n",
    "        for col in categorical_cols:\n",
    "            useful_fueatures.append(np.array(df[renaming_dict[col]].values, dtype=np.int32))\n",
    "\n",
    "    for cont in continuous_cols:\n",
    "        useful_fueatures.append(np.array(df[cont].values, dtype=np.float32))    \n",
    "    \n",
    "    if pred: \n",
    "        train_number = 1\n",
    "        batch_number = 1\n",
    "    else:\n",
    "        useful_fueatures.append(np.array(df[\"price\"].values, dtype=np.float32))\n",
    "        train_number = TRAIN_EPOCHS\n",
    "        batch_number = BATCH_SIZE\n",
    "        \n",
    "    A = tf.train.slice_input_producer(\n",
    "        tensor_list=useful_fueatures,\n",
    "        num_epochs=train_number,\n",
    "        shuffle= not pred,\n",
    "        capacity=BATCH_SIZE * 5\n",
    "    )\n",
    "\n",
    "    dataset_dict = dict()\n",
    "    \n",
    "    if use_all_features:\n",
    "        for i in range(len(A)):\n",
    "            if i < len(categorical_cols):\n",
    "                dataset_dict[renaming_dict[categorical_cols[i]]] = A[i]\n",
    "            elif i < len(categorical_cols) + len(continuous_cols):\n",
    "                dataset_dict[continuous_cols[i-len(categorical_cols)]] = A[i]\n",
    "    else:\n",
    "        for i in range(len(A)):\n",
    "            if i < len(continuous_cols):\n",
    "                dataset_dict[continuous_cols[i]] = A[i]\n",
    "            \n",
    "    if not pred:\n",
    "        dataset_dict['labels'] = A[-1]\n",
    "            \n",
    "    batch_dict = tf.train.batch(\n",
    "        dataset_dict,\n",
    "        batch_number,\n",
    "   )\n",
    "\n",
    "    if pred == False:\n",
    "        batch_labels = batch_dict.pop('labels')\n",
    "        return batch_dict, tf.reshape(batch_labels, [-1, 1]) \n",
    "    else:\n",
    "        return batch_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1a21ee0a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(learning_rate=lr)\n",
    "rconfig = tf.estimator.RunConfig(log_step_count_steps = 10)\n",
    "estimator_val = tf.estimator.Estimator(model_fn=make_model, params=hparams, config = rconfig)\n",
    "estimator_val.train(input_fn=lambda: input_fn(X_train), steps=TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.215470338632542"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_val   = list(estimator_val.predict(input_fn = lambda: input_fn(X_val, pred=True)))\n",
    "y_preds_val       = [int(x) for x in predictions_val]\n",
    "mean_absolute_precision_error(y_preds_val, X_val.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn=make_model, params=hparams, config = rconfig)\n",
    "estimator.train(input_fn=lambda: input_fn(train_renamed), steps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions   = list(estimator.predict(input_fn = lambda: input_fn(test_renamed, pred=True)))\n",
    "y_preds_test   = [int(x) for x in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/mohsenkiskani/Downloads/Ubaar/submissions/submission39.csv\"\n",
    "with open(filename,\"w+\") as outputfile:\n",
    "    outputfile.write(\"ID,price\\n\")\n",
    "    for i in range(len(y_preds_test)):\n",
    "        outputfile.write(str(test.ID[i])+\",\"+str(int(np.ceil(y_preds_test[i])))+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
